This dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 0.5M messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation.

The email dataset was later purchased by Leslie Kaelbling at MIT, and turned out to have a number of integrity problems. A number of folks at SRI, notably Melinda Gervasio, worked hard to correct these problems, and it is thanks to them (not me) that the dataset is available. The dataset here does not include attachments, and some messages have been deleted “as part of a redaction effort due to requests from affected employees”. Invalid email addresses were converted to something of the form user@enron.com whenever possible (i.e., recipient is specified in some parse-able format like “Doe, John” or “Mary K. Smith”) and to no_address@enron.com when no recipient was specified.
I get a number of questions about this corpus each week, which I am unable to answer, mostly because they deal with preparation issues and such that I just don’t know about. If you ask me a question and I don’t answer, please don’t feel slighted.

I am distributing this dataset as a resource for researchers who are interested in improving current email tools, or understanding how email is currently used. This data is valuable; to my knowledge it is the only substantial collection of “real” email that is public. The reason other datasets are not public is because of privacy concerns. In using this dataset, please be sensitive to the privacy of the people involved (and remember that many of these people were certainly not involved in any of the actions which precipitated the investigation.)

Downloads

Download is “about 400Mb, tarred and gzipped”.

Article Search API - NYTimes.com
Offsite — With the Article Search API, you can search New York Times articles from 1981 to today, retrieving headlines, abstracts, lead paragraphs, links to associated multimedia and other article metadata. Along with standard keyword searching, the API also offers faceted searching. The available facets include Times-specific fields such as sections, taxonomic classifiers and ...
Text Messages sent on 9/11/2001 (wikileaks.org)
Offsite — 9/11 tragedy pager intercepts. The following are more than half a million national US pager intercepts released by wikileaks.org. This covers the September 11 tragedy from 3am on the same day (Tuesday) until 3am the following day, a 24 hour period surrounding the attacks in New York and Washington. The fields presented are: Date Time Pager-Network Pager-number ...
Enron Email Dataset
Offsite — From the CALO Project at Carnegie-Mellon University a massive dataset of emails recovered from discovery documents in the Enron trials About This dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a ...
Email Data Sets
Offsite — Due to privacy issues, it is very hard to get a hold of large and realistic email corpora. Here you can find a few email data sets, as well as a dataset of news groups text – annotated with personal names spans. The email corpora given here were extracted from the Enron corpus, made public by the Federal agency Regulatory commission. As a second type of informal text, ...
Document Metadata Based on a Sample of Web Documents from the Open Directory
Offsite — DMOZ100k06 is a large research data set about document metadata based on a random sample of 100,000 web documents from the Open Directory combined with data retrieved from the social bookmarking service delicious.com, the content rating system ICRA, and the search engine Google. The data set is freely available for other research. Michael G. Noll
Reuters Spotlight - Article and Media API
Offsite — The Reuters Spotlight service provides Reuters.com content in the form of multimedia articles, pictures, videos and text news through a set standards based consumer XML APIs. The Spotlight service also provides an option to receive the content automatically annotated with rich semantic metadata.
German English Parallel Corpus &amp;quot;de-news&amp;quot;, Daily News 1996-2000
Offsite —
English-language Wikipedia[edit]
Dumps from any Wikimedia Foundation project: http://dumps.wikimedia.org/
English Wikipedia dumps in SQL and XML: http://dumps.wikimedia.org/enwiki/
pages-articles.xml.bz2 – Current revisions only, no talk or user pages. (This is probably the one you want. The size of the 13 February 2014 dump is approximately 9.85 GB compressed, 44 GB uncompressed).
As these files are quite large, please consider using a BitTorrent download to reduce the load on our servers. See meta:data dump torrents for more information and links to torrent files.
Additionally, there are numerous benefits using BitTorrent over a HTTP/FTP download. In an HTTP/FTP download, there is no way to compute the checksum of a file during downloading. Because of that limitation, data corruption is harder to prevent when downloading a large file. With BitTorrent downloads, the checksum of each block/chunk is computed using a SHA-1 hash during the download. If the computed hash doesn't match with the stored hash in the *.torrent file, that particular block/chunk is avoided. As a result, a BitTorrent download is generally more secure against data corruption than a regular HTTP/FTP download.[1]
pages-meta-current.xml.bz2 – Current revisions only, all pages (including talk)
abstract.xml.gz – page abstracts
all-titles-in-ns0.gz – Article titles only (with redirects)
SQL files for the pages and links are also available
All revisions, all pages: These files expand to multiple terabytes of text. Please only download these if you know you can cope with this quantity of data. Go to Latest Dumps and look out for all the files that have 'pages-meta-history' in their name.
To download a subset of the database in XML format, such as a specific category or a list of articles see: Special:Export, usage of which is described at Help:Export.
Wiki front-end software: MediaWiki [1].
Database backend software: You want to download MySQL.
Image dumps: See below.
Other languages[edit]
In the http://dumps.wikimedia.org/ directory you will find the latest SQL and XML dumps for the projects, not just English. For example, (others exist, just select the appropriate language code and the appropriate project):

Chinese Wikipedia dumps: http://dumps.wikimedia.org/zhwiki/
English Wikipedia dumps: http://dumps.wikimedia.org/enwiki/
French Wikipedia dumps: http://dumps.wikimedia.org/frwiki/
German Wikipedia dumps: http://dumps.wikimedia.org/dewiki/
Italian Wikipedia dumps: http://dumps.wikimedia.org/itwiki/
Japanese Wikipedia dumps: http://dumps.wikimedia.org/jawiki/
Polish Wikipedia dumps: http://dumps.wikimedia.org/plwiki/
Portuguese Wikipedia dumps: http://dumps.wikimedia.org/ptwiki/
Russian Wikipedia dumps: http://dumps.wikimedia.org/ruwiki/
Spanish Wikipedia dumps: http://dumps.wikimedia.org/eswiki/
Some other directories (e.g. simple, nostalgia) exist, with the same structure.

Where are the uploaded files (image, audio, video, etc., files)?[edit]
Images and other uploaded media are available from mirrors in addition to being served directly from Wikimedia servers. Bulk download is currently (as of September 2013) available from mirrors but not offered directly from Wikimedia servers. See the list of current mirrors. You should rsync from the mirror, then fill in the missing images from upload.wikimedia.org; when downloading from upload.wikimedia.org you should throttle yourself to 1 cache miss per second (you can check headers on a response to see if was a hit or miss and then back off when you get a miss) and you shouldn't use more than one or two simultaneous HTTP connections. In any case, make sure you have an accurate user agent string with contact info (email address) so ops can contact you if there's an issue. You should be getting checksums from the mediawiki API and verifying them. The API Etiquette page contains some guidelines, although not all of them apply (for example, because upload.wikimedia.org isn't MediaWiki, there is no maxlag parameter).

Unlike most article text, images are not necessarily licensed under the GFDL & CC-BY-SA-3.0. They may be under one of many free licenses, in the public domain, believed to be fair use, or even copyright infringements (which should be deleted). In particular, use of fair use images outside the context of Wikipedia or similar works may be illegal. Images under most licenses require a credit, and possibly other attached copyright information. This information is included in image description pages, which are part of the text dumps available from dumps.wikimedia.org. In conclusion, download these images at your own risk (Legal)

Dealing with compressed files[edit]
Compressed dump files are significantly compressed, thus after uncompressed will take up large amounts of drive space. The following are programs that can be used to uncompress bzip2 (.bz2) and .7z files.

Windows
Windows does not ship with a bzip2 decompressor program. The following can be used to decompress bzip2 files.

bzip2 (command-line) (from here) is available for free under a BSD license.
7-Zip is available for free under an LGPL license.
WinRAR
WinZip
Mac
OS X ships with the command-line bzip2 tool.

GNU/Linux
GNU/Linux ships with the command-line bzip2 tool.

BSD
Some BSD systems ship with the command-line bzip2 tool as part of the operating system. Others, such as OpenBSD, provide it as a package which must first be installed.

Notes
Some older versions of bzip2 may not be able to handle files larger than 2 GB, so make sure you have the latest version if you experience any problems.
Some older archives are compressed with gzip, which is compatible with PKZIP (the most common Windows format).
Dealing with large files[edit]
As files grow in size, so does the likelihood they will exceed some limitation of a computing device. Each operating system, file system, hard storage device, and software (application) has a maximum file size limit. Each one of these will likely have a different maximum file size limit, but the lowest limit of all of them will become the file size limit for a storage device.

The older the software in a computing device, the more likely it will have a 2 GB file limit somewhere in the system. This is due to older software using 32-bit integers for file indexing, which limits file sizes to 2^31 bytes (2 GB) (for signed integers), or 2^32 (4 GB) (for unsigned integers). Older C programming libraries have this 2 or 4 GB limitation, but the newer file libraries have been converted to 64-bit integers thus supporting file sizes up to 2^63 or 2^64 bytes (8 or 16 EB).

Before starting a download of a large file, check the storage device to ensure its file system can support files of such a large size, and check the amount of free space to ensure that it can hold the downloaded file.

File system limits[edit]
There are two limits for a file system: the file system size limit, and the file size limit. In general, since the file size limit is less than the file system limit, the larger file system limits are a moot point. A large percentage of users assume they can create files up to the size of their storage device, but are wrong in their assumption. For example, a 16 GB storage device formatted as FAT32 file system has a file limit of 4 GB for any single file. The following is a list of the most common file systems, and see Comparison of file systems for additional detailed information.

Windows
FAT16 supports files up to 4 GB. FAT16 is the factory format of smaller USB drives and all SD cards that are 2 GB or smaller.
FAT32 supports files up to 4 GB. FAT32 is the factory format of larger USB drives and all SDHC cards that are 4 GB or larger.
exFAT supports files up to 127 PB. exFAT is the factory format of all SDXC cards, but is incompatible with most flavors of UNIX due to licensing problems.
NTFS supports files up to 16 TB. NTFS is the default file system for Windows computers, including Windows 2000, Windows XP, and all their successors to date.
ReFS supports files up to 16 EB.
Mac
HFS+ supports files up to 8 EB on Mac OS X 10.2+ and iOS. HFS+ is the default file system for Mac computers.
Linux
ext2 and ext3 supports files up to 16 GB, but up to 2 TB with larger block sizes. See http://www.suse.com/~aj/linux_lfs.html for more information.
ext4 supports files up to 16 TB (using 4 KB block size). (limitation removed in e2fsprogs-1.42 (2012))
XFS supports files up to 8 EB.
ReiserFS supports files up to 1 EB (8 TB on 32-bit systems).
JFS supports files up to 4 PB.
Btrfs supports files up to 16 EB
NILFS supports files up to 8 EB
YAFFS2 supports files up to 2 GB.
FreeBSD
ZFS supports files up to 16 EB.
Operating system limits[edit]
Each operating system has internal file system limits for file size and drive size, which is independent of the file system or physical media. If the operating system has any limits lower than the file system or physical media, then the O/S limits will be the real limit.

Windows
For Windows 95/98/ME, there is a 4 GB limit for all file sizes.
For Windows XP, there is a 16 EiB limit for all file sizes.
For Windows 7, there is a 16 TiB limit for all file sizes.
For Windows 8/Server 2012, there is a 256 TiB limit for all file sizes.
Linux
For 32-bit Kernel 2.4.x systems, there is a 2 TB limit for all file systems.
For 64-bit Kernel 2.4.x systems, there is an 8 EB limit for all file systems.
For 32-bit Kernel 2.6.x systems without option CONFIG_LBD, there is a 2 TB limit for all file systems.
For 32-bit Kernel 2.6.x systems with option CONFIG_LBD and all 64-bit Kernel 2.6.x systems, there is an 8 ZB limit for all file systems.[2]
Google Android
Google Android is based upon Linux, which determines its base limits.

Internal Storage:
For Android 2.3 and later, uses the ext4 file system.[3]
For Android 2.2 and earlier, uses the YAFFS2 file system.
External Storage Slots:
All Android devices should support FAT16, FAT32, ext2 file systems.
Android 2.3 and later supports ext4 file system.
Apple iOS (see List of iOS devices)
All devices support HFS+ for internal storage. No devices have external storage slots.
Tips[edit]
Detect corrupted files
It is a good idea to check the MD5 sums (provided in a file in the download directory) to make sure your download was complete and accurate. You can check this by running the "md5sum" command on the files you downloaded. Given how large the files are, this may take some time to calculate. Due to the technical details of how files are stored, file sizes may be reported differently on different filesystems, and so are not necessarily reliable. Also, you may have experienced corruption during the download, though this is unlikely.

Reformatting external USB drives
If you plan to download Wikipedia Dump files to one computer and use an external USB Flash Drive or Hard Drive to copy them to other computers, then you will run into the 4 GB FAT32 file size limitation issue. To work around this issue, reformat the >4 GB USB Drive to a file system that supports larger file sizes. If you are working exclusively with Windows XP/Vista/7 computers, then reformat your USB Drive to NTFS file system. Windows ext2 driver

Linux and Unix
If you seem to be hitting the 2 GB limit, try using wget version 1.10 or greater, cURL version 7.11.1-1 or greater, or a recent version of lynx (using -dump). Also, you can resume downloads (for example wget -c).

Why not just retrieve data from [[w:|wikipedia.org]] at runtime?[edit]
Suppose you are building a piece of software that at certain points displays information that came from Wikipedia. If you want your program to display the information in a different way than can be seen in the live version, you'll probably need the wikicode that is used to enter it, instead of the finished HTML.

Also if you want to get all of the data, you'll probably want to transfer it in the most efficient way that's possible. The wikipedia.org servers need to do quite a bit of work to convert the wikicode into HTML. That's time consuming both for you and for the wikipedia.org servers, so simply spidering all pages is not the way to go.

To access any article in XML, one at a time, access Special:Export/Title of the article.

Read more about this at Special:Export.

Please be aware that live mirrors of Wikipedia that are dynamically loaded from the Wikimedia servers are prohibited. Please see Wikipedia:Mirrors and forks.

Please do not use a web crawler[edit]
Please do not use a web crawler to download large numbers of articles. Aggressive crawling of the server can cause a dramatic slow-down of Wikipedia.

Sample blocked crawler email[edit]
IP address nnn.nnn.nnn.nnn was retrieving up to 50 pages per second from wikipedia.org addresses. Robots.txt has a rate limit of one per second set using the Crawl-delay setting. Please respect that setting. If you must exceed it a little, do so only during the least busy times shown in our site load graphs at http://stats.wikimedia.org/EN/ChartsWikipediaZZ.htm. It's worth noting that to crawl the whole site at one hit per second will take several weeks. The originating IP is now blocked or will be shortly. Please contact us if you want it unblocked. Please don't try to circumvent it – we'll just block your whole IP range.
If you want information on how to get our content more efficiently, we offer a variety of methods, including weekly database dumps which you can load into MySQL and crawl locally at any rate you find convenient. Tools are also available which will do that for you as often as you like once you have the infrastructure in place. More details are available at http://en.wikipedia.org/wiki/Wikipedia:Database_download.
Instead of an email reply you may prefer to visit #mediawiki at irc.freenode.net to discuss your options with our team.
Note that the robots.txt currently has a commented out Crawl-delay:

 ## *at least* 1 second please. preferably more :D
 ## we're disabling this experimentally 11-09-2006
 #Crawl-delay: 1
Please be sure to use an intelligent non-zero delay regardless.

Doing Hadoop MapReduce on the Wikipedia current database dump[edit]
You can do Hadoop MapReduce queries on the current database dump, but you will need an extension to the InputRecordFormat to have each <page> </page> be a single mapper input. A working set of java methods (jobControl, mapper, reducer, and XmlInputRecordFormat) is available at Hadoop on the Wikipedia

Doing SQL queries on the current database dump[edit]
You can do SQL queries on the current database dump (as a replacement for the disabled Special:Asksql page).

Database schema[edit]
SQL schema[edit]
See also: mw:Manual:Database layout

The sql file used to initialize a MediaWiki database can be found here.

XML schema[edit]
The XML schema for each dump is defined at the top of the file. And also described in the MediaWiki export help page.

Help parsing dumps for use in scripts[edit]
Wikipedia:Computer help desk/ParseMediaWikiDump describes the Perl Parse::MediaWikiDump library, which can parse XML dumps.
Wikipedia preprocessor (wikiprep.pl) is a Perl script that preprocesses raw XML dumps and builds link tables, category hierarchies, collects anchor text for each article etc.
Wikipedia SQL dump parser is a .NET library to read MySQL dumps without the need to use MySQL database
Dictionary Builder is a Java program that can parse XML dumps and extract entries in files
Help importing dumps into MySQL[edit]
See:

mw:Manual:Importing XML dumps
m:Data_dumps
Static HTML tree dumps for mirroring or CD distribution[edit]
MediaWiki 1.5 includes routines to dump a wiki to HTML, rendering the HTML with the same parser used on a live wiki. As the following page states, putting one of these dumps on the web unmodified will constitute a trademark violation. They are intended for private viewing in an intranet or desktop installation.

If you want to draft a traditional website in Mediawiki and dump it to HTML format, you might want to try mw2html by User:Connelly.
If you'd like to help develop dump-to-static HTML tools, please drop us a note on the developers' mailing list.
Static HTML dumps are now available here, but are not current.
See also:

mw:Alternative parsers lists some other not working options for getting static HTML dumps
Wikipedia:Snapshots
Wikipedia:TomeRaider database
http://sdict.com hosts a January 2007 snapshot in the open source Sdictionary .dct format
http://ahuv.net/wikipedia hosts October 2010 processed snapshot in the freeware MDict .mdx format


Kiwix[edit]
Kiwix - last update of English Wikipedia was February 2014
Okawix[edit]
Provides Wikipedia pages with images.

Aard Dictionary[edit]
Offline Wikipedia reader. No images. Cross-Platform for Windows, Mac, Linux, Android, Maemo. Runs on rooted Nook and Sony PRS-T1 eBooks readers.

E-book[edit]
The wiki-as-ebook store provides ebooks created from a large set of Wikipedia articles with grayscale images for e-book-readers (2013).

Wikiviewer for Rockbox[edit]
The wikiviewer plugin for rockbox permits viewing converted Wikipedia dumps on many Rockbox devices. It needs a custom build and conversion of the wiki dumps using the instructions available at http://www.rockbox.org/tracker/4755 .The conversion recompresses the file and splits it into 1 GB files and an index file which all need to be in the same folder on the device or micro sd card.

Old dumps[edit]
The static version of Wikipedia created by Wikimedia: http://static.wikipedia.org/ Feb. 11, 2013 - This is apparently offline now. There was no content.
Wiki2static (site down as of October 2005) was an experimental program set up by User:Alfio to generate html dumps, inclusive of images, search function and alphabetical index. At the linked site experimental dumps and the script itself can be downloaded. As an example it was used to generate these copies of English WikiPedia 24 April 04, Simple WikiPedia 1 May 04(old database) format and English WikiPedia 24 July 04Simple WikiPedia 24 July 04, WikiPedia Francais 27 Juillet 2004 (new format). BozMo uses a version to generate periodic static copies at fixed reference.
Dynamic HTML generation from a local XML database dump[edit]
Instead of converting a database dump file to many pieces of static HTML, one can also use a dynamic HTML generator. Browsing a wiki page is just like browsing a Wiki site, but the content is fetched and converted from a local dump file upon request from the browser.

XOWA[edit]
XOWA is a free, open-source application that lets you download Wikipedia to your computer. Access all of Wikipedia offline -- without an internet connection! It is currently in the beta stage of development, but is functional. It is available for download here.

Features[edit]
Displays all articles from Wikipedia without an internet connection.
Download a complete, recent copy of English Wikipedia.
Display 4.4+ million articles in full HTML formatting.
Show images within an article. Access 3.7+ million images using the offline image databases.
Works with any Wikimedia wiki, including Wikipedia, Wiktionary, Wikisource, Wikiquote, Wikivoyage (also some non-wmf dumps)
Works with any non-English language wiki such as French Wikipedia, German Wikisource, Dutch Wikivoyage, etc.
Works with other specialized wikis such as Wikidata, Wikimedia Commons, Wikispecies, or any other MediaWiki generated dump
Set up over 660+ other wikis including:
English Wiktionary
English Wikisource
English Wikiquote
English Wikivoyage
Non-English wikis, such as French Wiktionary, German Wikisource, Dutch Wikivoyage
Wikidata
Wikimedia Commons
Wikispecies
... and many more!
Update your wiki whenever you want, using Wikimedia's database backups.
Navigate between offline wikis. Click on "Look up this word in Wiktionary" and instantly view the page in Wiktionary.
Edit articles to remove vandalism or errors.
Install to a flash memory card for portability to other machines.
Run on Windows, Linux and Mac OS X.
View the HTML for any wiki page.
Search for any page by title using a Wikipedia-like Search box.
Browse pages by alphabetical order using Special:AllPages.
Find a word on a page.
Access a history of viewed pages.
Bookmark your favorite pages.
Downloads images and other files on demand (when connected to the internet)
Sets up Simple Wikipedia in less than 5 minutes
Can be customized at many levels: from keyboard shortcuts to HTML layouts to internal options
Offline wikipedia reader[edit]
(for Mac OS X, GNU/Linux, FreeBSD/OpenBSD/NetBSD, and other Unices)

The offline-wikipedia project provides a very effective way to get an offline version of Wikipedia. It uses entirely free software. Packages are available for Ubuntu and soon for other Linux distributions.

Main features[edit]
Very fast searching
Keyword (actually, title words) based searching
Search produces multiple possible articles: you can choose amongst them
LaTeX based rendering for mathematical formulae
Minimal space requirements: the original .bz2 file plus the index
Very fast installation (a matter of hours) compared to loading the dump into MySQL
WikiFilter[edit]
WikiFilter is a program which allows you to browse over 100 dump files without visiting a Wiki site.

WikiFilter system requirements[edit]
A recent Windows version (WinXP is fine; Win98 and WinME won't work because they don't have NTFS support)
A fair bit of hard drive space (To install you will need about 12 - 15 Gigabytes; afterwards you will only need about 10 Gigabytes)
How to set up WikiFilter[edit]
Start downloading a Wikipedia database dump file such as an English Wikipedia dump. It is best to use a download manager such as GetRight so you can resume downloading the file even if your computer crashes or is shut down during the download.
Download XAMPPLITE from [2] (you must get the 1.5.0 version for it to work). Make sure to pick the file whose filename ends with .exe
Install/extract it to C:\XAMPPLITE.
Download WikiFilter 2.3 from this site: https://sourceforge.net/projects/wikifilter. You will have a choice of files to download, so make sure that you pick the 2.3 version. Extract it to C:\WIKIFILTER.
Copy the WikiFilter.so into your C:\XAMPPLITE\apache\modules folder.
Edit your C:\xampplite\apache\conf\httpd.conf file, and add the following line:
LoadModule WikiFilter_module "C:/XAMPPLITE/apache/modules/WikiFilter.so"
When your Wikipedia file has finished downloading, uncompress it into your C:\WIKIFILTER folder. (I used WinRAR http://www.rarlab.com/ demo version – BitZipper http://www.bitzipper.com/winrar.html works well too.)
Run WikiFilter (WikiIndex.exe), and go to your C:\WIKIFILTER folder, and drag and drop the XML file into the window, click Load, then Start.
After it finishes, exit the window, and go to your C:\XAMPPLITE folder. Run the setup_xampp.bat file to configure xampp.
When you finish with that, run the Xampp-Control.exe file, and start Apache.
Browse to http://localhost/wiki and see if it works
If it doesn't work, see the forums.
WikiTaxi[edit]
WikiTaxi is an offline-reader for wikis in MediaWiki format. It enables users to search and browse popular wikis like Wikipedia, Wikiquote, or WikiNews, without being connected to the Internet. WikiTaxi works well with different languages like English, German, Turkish, and others but has a problem with right-to-left language scripts. Doesn't allow to display images though.

WikiTaxi system requirements[edit]
Any Windows version starting from Windows 95 or later. Large File support (greater than 4 GB) for the huge wikis (English only at the time of this writing).
It also works on Linux with Wine.
16 MB RAM minimum for the WikiTaxi reader, 128 MB recommended for the importer (more for speed).
Storage space for the WikiTaxi database. This requires about 11.7 GiB for the English Wikipedia (as of 5 April 2011), 2 GB for German, less for other Wikis. These figures are likely to grow in the future.
WikiTaxi usage[edit]
Download WikiTaxi and extract to an empty folder. No installation is otherwise required.
Download the XML database dump (*.xml.bz2) of your favorite wiki.
Run WikiTaxi_Importer.exe to import the database dump into a WikiTaxi database. The importer takes care to uncompress the dump as it imports, so make sure to save your drive space and do not uncompress beforehand.
When the import is finished, start up WikiTaxi.exe and open the generated database file. You can start searching, browsing, and reading immediately.
After a successful import, the XML dump file is no longer needed and can be deleted to reclaim disk space.
To update an offline Wiki for WikiTaxi, download and import a more recent database dump.
For WikiTaxi reading, only two files are required: WikiTaxi.exe and the .taxi database. Copy them to any storage device (memory stick or memory card) or burn them to a CD or DVD and take your Wikipedia with you wherever you go!

BzReader and MzReader (for Windows)[edit]
BzReader is an offline Wikipedia reader with fast search capabilities. It renders the Wiki text into HTML and doesn't need to decompress the database. Requires Microsoft .NET framework 2.0.

MzReader by Mun206 works with (though is not affiliated with) BzReader, and allows further rendering of wikicode into better HTML, including an interpretation of the monobook skin. It aims to make pages more readable. Requires Microsoft Visual Basic 6.0 Runtime, which is not supplied with the download. Also requires Inet Control and Internet Controls (Internet Explorer 6 ActiveX), which are packaged with the download.

EPWING[edit]
Offline Wikipedia database in EPWING dictionary format, which is common and an out-dated JIS-standard in Japan, can be read including thumbnail images and tables with some rendering limitations, on any systems where a reader is available (Boookends). There are many free and commercial readers for Windows/Mobile, MacOSX/iOS (Mac, iPhone, iPad), Android, Unix/Linux/BSD, DOS, and Java-based browser applications (EPWING Viewers).

Mirror Building[edit]
WP-MIRROR[edit]
WP-MIRROR is a free utility for mirroring any desired set of WMF wikis. That is, it builds a wiki farm that the user can browse locally. WP-MIRROR builds a complete mirror with original size media files. WP-MIRROR is available for download.

See also[edit]
DBpedia
WikiReader
m:Export
m:Help:Downloading pages
m:Import
Meta:Data dumps#Other tools, for related tools, e.g. extractors and "dump readers"
Wikipedia:Wikipedia-CD/Download
Wikipedia:Size of Wikipedia
meta:Mirroring Wikimedia project XML dumps
meta:Static version tools
References[edit]
Jump up ^ Anthony Bellissimo; Brian N. Levine; Prashant Shenoy. "Exploring the Use of BitTorrent as the Basis for a Large Trace Repository". University of Massachusetts (USA). Archived from the original on 2013-12-20. Retrieved 2013-12-20.
Jump up ^ Large File Support in Linux
Jump up ^ Android 2.2 and before used YAFFS file system; December 14, 2010.
External links[edit]
Wikimedia Downloads.
Domas visits logs (read this!). Also, old data in the Internet Archive.
Wikimedia mailing lists archives.
User:Emijrp/Wikipedia Archive. An effort to find all the Wiki[mp]edia available data, and to encourage people to download it and save it around the globe.
Script to download all Wikipedia 7z dumps.
Apache > Hadoop > Core > docs > r1.2.1
Hadoop Hadoop
   
ProjectWikiHadoop 1.2.1 Documentation
Last Published: 08/04/2013 08:13:21
 
Getting Started
Guides
MapReduce
MapReduce Tutorial
Hadoop Streaming
Hadoop Commands
DistCp
DistCp Version 2
Vaidya
Hadoop Archives
Gridmix
Rumen
Capacity Scheduler
Fair Scheduler
Hod Scheduler
HDFS
Common
Miscellaneous

PDF -icon
PDF
MapReduce Tutorial
Purpose
Prerequisites
Overview
Inputs and Outputs
Example: WordCount v1.0
Source Code
Usage
Walk-through
MapReduce - User Interfaces
Payload
Mapper
Reducer
Partitioner
Reporter
OutputCollector
Job Configuration
Task Execution & Environment
Memory Management
Map Parameters
Shuffle/Reduce Parameters
Directory Structure
Task JVM Reuse
Configured Parameters
Task Logs
Distributing Libraries
Job Submission and Monitoring
Job Authorization
Job Control
Job Credentials
Job Input
InputSplit
RecordReader
Job Output
OutputCommitter
Task Side-Effect Files
RecordWriter
Other Useful Features
Submitting Jobs to Queues
Counters
DistributedCache
Tool
IsolationRunner
Profiling
Debugging
JobControl
Data Compression
Skipping Bad Records
Example: WordCount v2.0
Source Code
Sample Runs
Highlights
Purpose
This document comprehensively describes all user-facing facets of the Hadoop MapReduce framework and serves as a tutorial.

Prerequisites
Ensure that Hadoop is installed, configured and is running. More details:

Single Node Setup for first-time users.
Cluster Setup for large, distributed clusters.
Overview
Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.

A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks.

Typically the compute nodes and the storage nodes are the same, that is, the MapReduce framework and the Hadoop Distributed File System (see HDFS Architecture Guide) are running on the same set of nodes. This configuration allows the framework to effectively schedule tasks on the nodes where data is already present, resulting in very high aggregate bandwidth across the cluster.

The MapReduce framework consists of a single master JobTracker and one slave TaskTracker per cluster-node. The master is responsible for scheduling the jobs' component tasks on the slaves, monitoring them and re-executing the failed tasks. The slaves execute the tasks as directed by the master.

Minimally, applications specify the input/output locations and supply map and reduce functions via implementations of appropriate interfaces and/or abstract-classes. These, and other job parameters, comprise the job configuration. The Hadoop job client then submits the job (jar/executable etc.) and configuration to the JobTracker which then assumes the responsibility of distributing the software/configuration to the slaves, scheduling tasks and monitoring them, providing status and diagnostic information to the job-client.

Although the Hadoop framework is implemented in JavaTM, MapReduce applications need not be written in Java.

Hadoop Streaming is a utility which allows users to create and run jobs with any executables (e.g. shell utilities) as the mapper and/or the reducer.
Hadoop Pipes is a SWIG- compatible C++ API to implement MapReduce applications (non JNITM based).
Inputs and Outputs
The MapReduce framework operates exclusively on <key, value> pairs, that is, the framework views the input to the job as a set of <key, value> pairs and produces a set of <key, value> pairs as the output of the job, conceivably of different types.

The key and value classes have to be serializable by the framework and hence need to implement the Writable interface. Additionally, the key classes have to implement the WritableComparable interface to facilitate sorting by the framework.

Input and Output types of a MapReduce job:

(input) <k1, v1> -> map -> <k2, v2> -> combine -> <k2, v2> -> reduce -> <k3, v3> (output)

Example: WordCount v1.0
Before we jump into the details, lets walk through an example MapReduce application to get a flavour for how they work.

WordCount is a simple application that counts the number of occurences of each word in a given input set.

This works with a local-standalone, pseudo-distributed or fully-distributed Hadoop installation (Single Node Setup).

Source Code
WordCount.java
1.	package org.myorg;
2.	
3.	import java.io.IOException;
4.	import java.util.*;
5.	
6.	import org.apache.hadoop.fs.Path;
7.	import org.apache.hadoop.conf.*;
8.	import org.apache.hadoop.io.*;
9.	import org.apache.hadoop.mapred.*;
10.	import org.apache.hadoop.util.*;
11.	
12.	public class WordCount {
13.	
14.	    public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {
15.	      private final static IntWritable one = new IntWritable(1);
16.	      private Text word = new Text();
17.	
18.	      public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {
19.	        String line = value.toString();
20.	        StringTokenizer tokenizer = new StringTokenizer(line);
21.	        while (tokenizer.hasMoreTokens()) {
22.	          word.set(tokenizer.nextToken());
23.	          output.collect(word, one);
24.	        }
25.	      }
26.	    }
27.	
28.	    public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {
29.	      public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {
30.	        int sum = 0;
31.	        while (values.hasNext()) {
32.	          sum += values.next().get();
33.	        }
34.	        output.collect(key, new IntWritable(sum));
35.	      }
36.	    }
37.	
38.	    public static void main(String[] args) throws Exception {
39.	      JobConf conf = new JobConf(WordCount.class);
40.	      conf.setJobName("wordcount");
41.	
42.	      conf.setOutputKeyClass(Text.class);
43.	      conf.setOutputValueClass(IntWritable.class);
44.	
45.	      conf.setMapperClass(Map.class);
46.	      conf.setCombinerClass(Reduce.class);
47.	      conf.setReducerClass(Reduce.class);
48.	
49.	      conf.setInputFormat(TextInputFormat.class);
50.	      conf.setOutputFormat(TextOutputFormat.class);
51.	
52.	      FileInputFormat.setInputPaths(conf, new Path(args[0]));
53.	      FileOutputFormat.setOutputPath(conf, new Path(args[1]));
54.	
55.	      JobClient.runJob(conf);
57.	    }
58.	}
59.	
Usage
Assuming HADOOP_HOME is the root of the installation and HADOOP_VERSION is the Hadoop version installed, compile WordCount.java and create a jar:

$ mkdir wordcount_classes 
$ javac -classpath ${HADOOP_HOME}/hadoop-${HADOOP_VERSION}-core.jar -d wordcount_classes WordCount.java 
$ jar -cvf /usr/joe/wordcount.jar -C wordcount_classes/ .

Assuming that:

/usr/joe/wordcount/input - input directory in HDFS
/usr/joe/wordcount/output - output directory in HDFS
Sample text-files as input:

$ bin/hadoop dfs -ls /usr/joe/wordcount/input/ 
/usr/joe/wordcount/input/file01 
/usr/joe/wordcount/input/file02 

$ bin/hadoop dfs -cat /usr/joe/wordcount/input/file01 
Hello World Bye World 

$ bin/hadoop dfs -cat /usr/joe/wordcount/input/file02 
Hello Hadoop Goodbye Hadoop

Run the application:

$ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount /usr/joe/wordcount/input /usr/joe/wordcount/output

Output:

$ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 
Bye 1 
Goodbye 1 
Hadoop 2 
Hello 2 
World 2 
Applications can specify a comma separated list of paths which would be present in the current working directory of the task using the option -files. The -libjars option allows applications to add jars to the classpaths of the maps and reduces. The option -archives allows them to pass comma separated list of archives as arguments. These archives are unarchived and a link with name of the archive is created in the current working directory of tasks. More details about the command line options are available at Commands Guide.

Running wordcount example with -libjars, -files and -archives: 
hadoop jar hadoop-examples.jar wordcount -files cachefile.txt -libjars mylib.jar -archives myarchive.zip input output Here, myarchive.zip will be placed and unzipped into a directory by the name "myarchive.zip".

Users can specify a different symbolic name for files and archives passed through -files and -archives option, using #.

For example, hadoop jar hadoop-examples.jar wordcount -files dir1/dict.txt#dict1,dir2/dict.txt#dict2 -archives mytar.tgz#tgzdir input output Here, the files dir1/dict.txt and dir2/dict.txt can be accessed by tasks using the symbolic names dict1 and dict2 respectively. The archive mytar.tgz will be placed and unarchived into a directory by the name "tgzdir".

Walk-through
The WordCount application is quite straight-forward.

The Mapper implementation (lines 14-26), via the map method (lines 18-25), processes one line at a time, as provided by the specified TextInputFormat (line 49). It then splits the line into tokens separated by whitespaces, via the StringTokenizer, and emits a key-value pair of < <word>, 1>.

For the given sample input the first map emits:
< Hello, 1> 
< World, 1> 
< Bye, 1> 
< World, 1> 
The second map emits:
< Hello, 1> 
< Hadoop, 1> 
< Goodbye, 1> 
< Hadoop, 1> 
We'll learn more about the number of maps spawned for a given job, and how to control them in a fine-grained manner, a bit later in the tutorial.

WordCount also specifies a combiner (line 46). Hence, the output of each map is passed through the local combiner (which is same as the Reducer as per the job configuration) for local aggregation, after being sorted on the keys.

The output of the first map:
< Bye, 1> 
< Hello, 1> 
< World, 2> 
The output of the second map:
< Goodbye, 1> 
< Hadoop, 2> 
< Hello, 1> 
The Reducer implementation (lines 28-36), via the reduce method (lines 29-35) just sums up the values, which are the occurence counts for each key (i.e. words in this example).

Thus the output of the job is:
< Bye, 1> 
< Goodbye, 1> 
< Hadoop, 2> 
< Hello, 2> 
< World, 2> 
The run method specifies various facets of the job, such as the input/output paths (passed via the command line), key/value types, input/output formats etc., in the JobConf. It then calls the JobClient.runJob (line 55) to submit the and monitor its progress.

We'll learn more about JobConf, JobClient, Tool and other interfaces and classes a bit later in the tutorial.

MapReduce - User Interfaces
This section provides a reasonable amount of detail on every user-facing aspect of the MapReduce framework. This should help users implement, configure and tune their jobs in a fine-grained manner. However, please note that the javadoc for each class/interface remains the most comprehensive documentation available; this is only meant to be a tutorial.

Let us first take the Mapper and Reducer interfaces. Applications typically implement them to provide the map and reduce methods.

We will then discuss other core interfaces including JobConf, JobClient, Partitioner, OutputCollector, Reporter, InputFormat, OutputFormat, OutputCommitter and others.

Finally, we will wrap up by discussing some useful features of the framework such as the DistributedCache, IsolationRunner etc.

Payload
Applications typically implement the Mapper and Reducer interfaces to provide the map and reduce methods. These form the core of the job.

Mapper
Mapper maps input key/value pairs to a set of intermediate key/value pairs.

Maps are the individual tasks that transform input records into intermediate records. The transformed intermediate records do not need to be of the same type as the input records. A given input pair may map to zero or many output pairs.

The Hadoop MapReduce framework spawns one map task for each InputSplit generated by the InputFormat for the job.

Overall, Mapper implementations are passed the JobConf for the job via the JobConfigurable.configure(JobConf) method and override it to initialize themselves. The framework then calls map(WritableComparable, Writable, OutputCollector, Reporter) for each key/value pair in the InputSplit for that task. Applications can then override the Closeable.close() method to perform any required cleanup.

Output pairs do not need to be of the same types as input pairs. A given input pair may map to zero or many output pairs. Output pairs are collected with calls to OutputCollector.collect(WritableComparable,Writable).

Applications can use the Reporter to report progress, set application-level status messages and update Counters, or just indicate that they are alive.

All intermediate values associated with a given output key are subsequently grouped by the framework, and passed to the Reducer(s) to determine the final output. Users can control the grouping by specifying a Comparator via JobConf.setOutputKeyComparatorClass(Class).

The Mapper outputs are sorted and then partitioned per Reducer. The total number of partitions is the same as the number of reduce tasks for the job. Users can control which keys (and hence records) go to which Reducer by implementing a custom Partitioner.

Users can optionally specify a combiner, via JobConf.setCombinerClass(Class), to perform local aggregation of the intermediate outputs, which helps to cut down the amount of data transferred from the Mapper to the Reducer.

The intermediate, sorted outputs are always stored in a simple (key-len, key, value-len, value) format. Applications can control if, and how, the intermediate outputs are to be compressed and the CompressionCodec to be used via the JobConf.

How Many Maps?
The number of maps is usually driven by the total size of the inputs, that is, the total number of blocks of the input files.

The right level of parallelism for maps seems to be around 10-100 maps per-node, although it has been set up to 300 maps for very cpu-light map tasks. Task setup takes awhile, so it is best if the maps take at least a minute to execute.

Thus, if you expect 10TB of input data and have a blocksize of 128MB, you'll end up with 82,000 maps, unless setNumMapTasks(int) (which only provides a hint to the framework) is used to set it even higher.

Reducer
Reducer reduces a set of intermediate values which share a key to a smaller set of values.

The number of reduces for the job is set by the user via JobConf.setNumReduceTasks(int).

Overall, Reducer implementations are passed the JobConf for the job via the JobConfigurable.configure(JobConf) method and can override it to initialize themselves. The framework then calls reduce(WritableComparable, Iterator, OutputCollector, Reporter) method for each <key, (list of values)> pair in the grouped inputs. Applications can then override the Closeable.close() method to perform any required cleanup.

Reducer has 3 primary phases: shuffle, sort and reduce.

Shuffle
Input to the Reducer is the sorted output of the mappers. In this phase the framework fetches the relevant partition of the output of all the mappers, via HTTP.

Sort
The framework groups Reducer inputs by keys (since different mappers may have output the same key) in this stage.

The shuffle and sort phases occur simultaneously; while map-outputs are being fetched they are merged.

Secondary Sort
If equivalence rules for grouping the intermediate keys are required to be different from those for grouping keys before reduction, then one may specify a Comparator via JobConf.setOutputValueGroupingComparator(Class). Since JobConf.setOutputKeyComparatorClass(Class) can be used to control how intermediate keys are grouped, these can be used in conjunction to simulate secondary sort on values.

Reduce
In this phase the reduce(WritableComparable, Iterator, OutputCollector, Reporter) method is called for each <key, (list of values)> pair in the grouped inputs.

The output of the reduce task is typically written to the FileSystem via OutputCollector.collect(WritableComparable, Writable).

Applications can use the Reporter to report progress, set application-level status messages and update Counters, or just indicate that they are alive.

The output of the Reducer is not sorted.

How Many Reduces?
The right number of reduces seems to be 0.95 or 1.75 multiplied by (<no. of nodes> * mapred.tasktracker.reduce.tasks.maximum).

With 0.95 all of the reduces can launch immediately and start transfering map outputs as the maps finish. With 1.75 the faster nodes will finish their first round of reduces and launch a second wave of reduces doing a much better job of load balancing.

Increasing the number of reduces increases the framework overhead, but increases load balancing and lowers the cost of failures.

The scaling factors above are slightly less than whole numbers to reserve a few reduce slots in the framework for speculative-tasks and failed tasks.

Reducer NONE
It is legal to set the number of reduce-tasks to zero if no reduction is desired.

In this case the outputs of the map-tasks go directly to the FileSystem, into the output path set by setOutputPath(Path). The framework does not sort the map-outputs before writing them out to the FileSystem.

Partitioner
Partitioner partitions the key space.

Partitioner controls the partitioning of the keys of the intermediate map-outputs. The key (or a subset of the key) is used to derive the partition, typically by a hash function. The total number of partitions is the same as the number of reduce tasks for the job. Hence this controls which of the m reduce tasks the intermediate key (and hence the record) is sent to for reduction.

HashPartitioner is the default Partitioner.

Reporter
Reporter is a facility for MapReduce applications to report progress, set application-level status messages and update Counters.

Mapper and Reducer implementations can use the Reporter to report progress or just indicate that they are alive. In scenarios where the application takes a significant amount of time to process individual key/value pairs, this is crucial since the framework might assume that the task has timed-out and kill that task. Another way to avoid this is to set the configuration parameter mapred.task.timeout to a high-enough value (or even set it to zero for no time-outs).

Applications can also update Counters using the Reporter.

OutputCollector
OutputCollector is a generalization of the facility provided by the MapReduce framework to collect data output by the Mapper or the Reducer (either the intermediate outputs or the output of the job).

Hadoop MapReduce comes bundled with a library of generally useful mappers, reducers, and partitioners.

Job Configuration
JobConf represents a MapReduce job configuration.

JobConf is the primary interface for a user to describe a MapReduce job to the Hadoop framework for execution. The framework tries to faithfully execute the job as described by JobConf, however:

f Some configuration parameters may have been marked as final by administrators and hence cannot be altered.
While some job parameters are straight-forward to set (e.g. setNumReduceTasks(int)), other parameters interact subtly with the rest of the framework and/or job configuration and are more complex to set (e.g. setNumMapTasks(int)).
JobConf is typically used to specify the Mapper, combiner (if any), Partitioner, Reducer, InputFormat, OutputFormat and OutputCommitter implementations. JobConf also indicates the set of input files (setInputPaths(JobConf, Path...) /addInputPath(JobConf, Path)) and (setInputPaths(JobConf, String) /addInputPaths(JobConf, String)) and where the output files should be written (setOutputPath(Path)).

Optionally, JobConf is used to specify other advanced facets of the job such as the Comparator to be used, files to be put in the DistributedCache, whether intermediate and/or job outputs are to be compressed (and how), debugging via user-provided scripts (setMapDebugScript(String)/setReduceDebugScript(String)) , whether job tasks can be executed in a speculative manner (setMapSpeculativeExecution(boolean))/(setReduceSpeculativeExecution(boolean)) , maximum number of attempts per task (setMaxMapAttempts(int)/setMaxReduceAttempts(int)) , percentage of tasks failure which can be tolerated by the job (setMaxMapTaskFailuresPercent(int)/setMaxReduceTaskFailuresPercent(int)) etc.

Of course, users can use set(String, String)/get(String, String) to set/get arbitrary parameters needed by applications. However, use the DistributedCache for large amounts of (read-only) data.

Task Execution & Environment
The TaskTracker executes the Mapper/ Reducer task as a child process in a separate jvm.

The child-task inherits the environment of the parent TaskTracker. The user can specify additional options to the child-jvm via the mapred.{map|reduce}.child.java.opts configuration parameter in the JobConf such as non-standard paths for the run-time linker to search shared libraries via -Djava.library.path=<> etc. If the mapred.{map|reduce}.child.java.opts parameters contains the symbol @taskid@ it is interpolated with value of taskid of the MapReduce task.

Here is an example with multiple arguments and substitutions, showing jvm GC logging, and start of a passwordless JVM JMX agent so that it can connect with jconsole and the likes to watch child memory, threads and get thread dumps. It also sets the maximum heap-size of the map and reduce child jvm to 512MB & 1024MB respectively. It also adds an additional path to the java.library.path of the child-jvm.

<property> 
  <name>mapred.map.child.java.opts</name> 
  <value> 
     -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc 
     -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false 
  </value> 
</property>

<property> 
  <name>mapred.reduce.child.java.opts</name> 
  <value> 
     -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc 
     -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false 
  </value> 
</property>

Memory Management
Users/admins can also specify the maximum virtual memory of the launched child-task, and any sub-process it launches recursively, using mapred.{map|reduce}.child.ulimit. Note that the value set here is a per process limit. The value for mapred.{map|reduce}.child.ulimit should be specified in kilo bytes (KB). And also the value must be greater than or equal to the -Xmx passed to JavaVM, else the VM might not start.

Note: mapred.{map|reduce}.child.java.opts are used only for configuring the launched child tasks from task tracker. Configuring the memory options for daemons is documented in Configuring the Environment of the Hadoop Daemons.

The memory available to some parts of the framework is also configurable. In map and reduce tasks, performance may be influenced by adjusting parameters influencing the concurrency of operations and the frequency with which data will hit disk. Monitoring the filesystem counters for a job- particularly relative to byte counts from the map and into the reduce- is invaluable to the tuning of these parameters.

Users can choose to override default limits of Virtual Memory and RAM enforced by the task tracker, if memory management is enabled. Users can set the following parameter per job:

Name	Type	Description
mapred.task.maxvmem	int	A number, in bytes, that represents the maximum Virtual Memory task-limit for each task of the job. A task will be killed if it consumes more Virtual Memory than this number.
mapred.task.maxpmem	int	A number, in bytes, that represents the maximum RAM task-limit for each task of the job. This number can be optionally used by Schedulers to prevent over-scheduling of tasks on a node based on RAM needs.
Map Parameters
A record emitted from a map will be serialized into a buffer and metadata will be stored into accounting buffers. As described in the following options, when either the serialization buffer or the metadata exceed a threshold, the contents of the buffers will be sorted and written to disk in the background while the map continues to output records. If either buffer fills completely while the spill is in progress, the map thread will block. When the map is finished, any remaining records are written to disk and all on-disk segments are merged into a single file. Minimizing the number of spills to disk can decrease map time, but a larger buffer also decreases the memory available to the mapper.

Name	Type	Description
io.sort.mb	int	The cumulative size of the serialization and accounting buffers storing records emitted from the map, in megabytes.
io.sort.record.percent	float	The ratio of serialization to accounting space can be adjusted. Each serialized record requires 16 bytes of accounting information in addition to its serialized size to effect the sort. This percentage of space allocated from io.sort.mb affects the probability of a spill to disk being caused by either exhaustion of the serialization buffer or the accounting space. Clearly, for a map outputting small records, a higher value than the default will likely decrease the number of spills to disk.
io.sort.spill.percent	float	This is the threshold for the accounting and serialization buffers. When this percentage of either buffer has filled, their contents will be spilled to disk in the background. Let io.sort.record.percent be r, io.sort.mb be x, and this value be q. The maximum number of records collected before the collection thread will spill is r * x * q * 2^16. Note that a higher value may decrease the number of- or even eliminate- merges, but will also increase the probability of the map task getting blocked. The lowest average map times are usually obtained by accurately estimating the size of the map output and preventing multiple spills.
Other notes

If either spill threshold is exceeded while a spill is in progress, collection will continue until the spill is finished. For example, if io.sort.buffer.spill.percent is set to 0.33, and the remainder of the buffer is filled while the spill runs, the next spill will include all the collected records, or 0.66 of the buffer, and will not generate additional spills. In other words, the thresholds are defining triggers, not blocking.
A record larger than the serialization buffer will first trigger a spill, then be spilled to a separate file. It is undefined whether or not this record will first pass through the combiner.
Shuffle/Reduce Parameters
As described previously, each reduce fetches the output assigned to it by the Partitioner via HTTP into memory and periodically merges these outputs to disk. If intermediate compression of map outputs is turned on, each output is decompressed into memory. The following options affect the frequency of these merges to disk prior to the reduce and the memory allocated to map output during the reduce.

Name	Type	Description
io.sort.factor	int	Specifies the number of segments on disk to be merged at the same time. It limits the number of open files and compression codecs during the merge. If the number of files exceeds this limit, the merge will proceed in several passes. Though this limit also applies to the map, most jobs should be configured so that hitting this limit is unlikely there.
mapred.inmem.merge.threshold	int	The number of sorted map outputs fetched into memory before being merged to disk. Like the spill thresholds in the preceding note, this is not defining a unit of partition, but a trigger. In practice, this is usually set very high (1000) or disabled (0), since merging in-memory segments is often less expensive than merging from disk (see notes following this table). This threshold influences only the frequency of in-memory merges during the shuffle.
mapred.job.shuffle.merge.percent	float	The memory threshold for fetched map outputs before an in-memory merge is started, expressed as a percentage of memory allocated to storing map outputs in memory. Since map outputs that can't fit in memory can be stalled, setting this high may decrease parallelism between the fetch and merge. Conversely, values as high as 1.0 have been effective for reduces whose input can fit entirely in memory. This parameter influences only the frequency of in-memory merges during the shuffle.
mapred.job.shuffle.input.buffer.percent	float	The percentage of memory- relative to the maximum heapsize as typically specified in mapred.reduce.child.java.opts- that can be allocated to storing map outputs during the shuffle. Though some memory should be set aside for the framework, in general it is advantageous to set this high enough to store large and numerous map outputs.
mapred.job.reduce.input.buffer.percent	float	The percentage of memory relative to the maximum heapsize in which map outputs may be retained during the reduce. When the reduce begins, map outputs will be merged to disk until those that remain are under the resource limit this defines. By default, all map outputs are merged to disk before the reduce begins to maximize the memory available to the reduce. For less memory-intensive reduces, this should be increased to avoid trips to disk.
Other notes

If a map output is larger than 25 percent of the memory allocated to copying map outputs, it will be written directly to disk without first staging through memory.
When running with a combiner, the reasoning about high merge thresholds and large buffers may not hold. For merges started before all map outputs have been fetched, the combiner is run while spilling to disk. In some cases, one can obtain better reduce times by spending resources combining map outputs- making disk spills small and parallelizing spilling and fetching- rather than aggressively increasing buffer sizes.
When merging in-memory map outputs to disk to begin the reduce, if an intermediate merge is necessary because there are segments to spill and at least io.sort.factor segments already on disk, the in-memory map outputs will be part of the intermediate merge.
Directory Structure
The task tracker has local directory, ${mapred.local.dir}/taskTracker/ to create localized cache and localized job. It can define multiple local directories (spanning multiple disks) and then each filename is assigned to a semi-random local directory. When the job starts, task tracker creates a localized job directory relative to the local directory specified in the configuration. Thus the task tracker directory structure looks as following:

${mapred.local.dir}/taskTracker/distcache/ : The public distributed cache for the jobs of all users. This directory holds the localized public distributed cache. Thus localized public distributed cache is shared among all the tasks and jobs of all users.
${mapred.local.dir}/taskTracker/$user/distcache/ : The private distributed cache for the jobs of the specific user. This directory holds the localized private distributed cache. Thus localized private distributed cache is shared among all the tasks and jobs of the specific user only. It is not accessible to jobs of other users.
${mapred.local.dir}/taskTracker/$user/jobcache/$jobid/ : The localized job directory
${mapred.local.dir}/taskTracker/$user/jobcache/$jobid/work/ : The job-specific shared directory. The tasks can use this space as scratch space and share files among them. This directory is exposed to the users through the configuration property job.local.dir. The directory can accessed through the API JobConf.getJobLocalDir(). It is available as System property also. So, users (streaming etc.) can call System.getProperty("job.local.dir") to access the directory.
${mapred.local.dir}/taskTracker/$user/jobcache/$jobid/jars/ : The jars directory, which has the job jar file and expanded jar. The job.jar is the application's jar file that is automatically distributed to each machine. It is expanded in jars directory before the tasks for the job start. The job.jar location is accessible to the application through the api JobConf.getJar() . To access the unjarred directory, JobConf.getJar().getParent() can be called.
${mapred.local.dir}/taskTracker/$user/jobcache/$jobid/job.xml : The job.xml file, the generic job configuration, localized for the job.
${mapred.local.dir}/taskTracker/$user/jobcache/$jobid/$taskid : The task directory for each task attempt. Each task directory again has the following structure :
${mapred.local.dir}/taskTracker/$user/jobcache/$jobid/$taskid/job.xml : A job.xml file, task localized job configuration, Task localization means that properties have been set that are specific to this particular task within the job. The properties localized for each task are described below.
${mapred.local.dir}/taskTracker/$user/jobcache/$jobid/$taskid/output : A directory for intermediate output files. This contains the temporary map reduce data generated by the framework such as map output files etc.
${mapred.local.dir}/taskTracker/$user/jobcache/$jobid/$taskid/work : The current working directory of the task. With jvm reuse enabled for tasks, this directory will be the directory on which the jvm has started
${mapred.local.dir}/taskTracker/$user/jobcache/$jobid/$taskid/work/tmp : The temporary directory for the task. (User can specify the property mapred.child.tmp to set the value of temporary directory for map and reduce tasks. This defaults to ./tmp. If the value is not an absolute path, it is prepended with task's working directory. Otherwise, it is directly assigned. The directory will be created if it doesn't exist. Then, the child java tasks are executed with option -Djava.io.tmpdir='the absolute path of the tmp dir'. Pipes and streaming are set with environment variable, TMPDIR='the absolute path of the tmp dir'). This directory is created, if mapred.child.tmp has the value ./tmp
Task JVM Reuse
Jobs can enable task JVMs to be reused by specifying the job configuration mapred.job.reuse.jvm.num.tasks. If the value is 1 (the default), then JVMs are not reused (i.e. 1 task per JVM). If it is -1, there is no limit to the number of tasks a JVM can run (of the same job). One can also specify some value greater than 1 using the api JobConf.setNumTasksToExecutePerJvm(int)

Configured Parameters
The following properties are localized in the job configuration for each task's execution:

Name	Type	Description
mapred.job.id	String	The job id
mapred.jar	String	job.jar location in job directory
job.local.dir	 String	 The job specific shared scratch space
mapred.tip.id	 String	 The task id
mapred.task.id	 String	 The task attempt id
mapred.task.is.map	 boolean	Is this a map task
mapred.task.partition	 int	The id of the task within the job
map.input.file	 String	 The filename that the map is reading from
map.input.start	 long	 The offset of the start of the map input split
map.input.length	long	The number of bytes in the map input split
mapred.work.output.dir	 String	The task's temporary output directory
Note: During the execution of a streaming job, the names of the "mapred" parameters are transformed. The dots ( . ) become underscores ( _ ). For example, mapred.job.id becomes mapred_job_id and mapred.jar becomes mapred_jar. To get the values in a streaming job's mapper/reducer use the parameter names with the underscores.

Task Logs
The standard output (stdout) and error (stderr) streams of the task are read by the TaskTracker and logged to ${HADOOP_LOG_DIR}/userlogs

Distributing Libraries
The DistributedCache can also be used to distribute both jars and native libraries for use in the map and/or reduce tasks. The child-jvm always has its current working directory added to the java.library.path and LD_LIBRARY_PATH. And hence the cached libraries can be loaded via System.loadLibrary or System.load. More details on how to load shared libraries through distributed cache are documented at native_libraries.html

Job Submission and Monitoring
JobClient is the primary interface by which user-job interacts with the JobTracker.

JobClient provides facilities to submit jobs, track their progress, access component-tasks' reports and logs, get the MapReduce cluster's status information and so on.

The job submission process involves:

Checking the input and output specifications of the job.
Computing the InputSplit values for the job.
Setting up the requisite accounting information for the DistributedCache of the job, if necessary.
Copying the job's jar and configuration to the MapReduce system directory on the FileSystem.
Submitting the job to the JobTracker and optionally monitoring it's status.
Job history files are also logged to user specified directory hadoop.job.history.user.location which defaults to job output directory. The files are stored in "_logs/history/" in the specified directory. Hence, by default they will be in mapred.output.dir/_logs/history. User can stop logging by giving the value none for hadoop.job.history.user.location

User can view the history logs summary in specified directory using the following command 
$ bin/hadoop job -history output-dir 
This command will print job details, failed and killed tip details. 
More details about the job such as successful tasks and task attempts made for each task can be viewed using the following command 
$ bin/hadoop job -history all output-dir 
User can use OutputLogFilter to filter log files from the output directory listing.

Normally the user creates the application, describes various facets of the job via JobConf, and then uses the JobClient to submit the job and monitor its progress.

Job Authorization
Job level authorization and queue level authorization are enabled on the cluster, if the configuration mapred.acls.enabled is set to true. When enabled, access control checks are done by (a) the JobTracker before allowing users to submit jobs to queues and administering these jobs and (b) by the JobTracker and the TaskTracker before allowing users to view job details or to modify a job using MapReduce APIs, CLI or web user interfaces.

A job submitter can specify access control lists for viewing or modifying a job via the configuration properties mapreduce.job.acl-view-job and mapreduce.job.acl-modify-job respectively. By default, nobody is given access in these properties.

However, irrespective of the job ACLs configured, a job's owner, the superuser and cluster administrators (mapreduce.cluster.administrators) and queue administrators of the queue to which the job was submitted to (mapred.queue.queue-name.acl-administer-jobs) always have access to view and modify a job.

A job view ACL authorizes users against the configured mapreduce.job.acl-view-job before returning possibly sensitive information about a job, like:

job level counters
task level counters
tasks's diagnostic information
task logs displayed on the TaskTracker web UI
job.xml showed by the JobTracker's web UI
Other information about a job, like its status and its profile, is accessible to all users, without requiring authorization.

A job modification ACL authorizes users against the configured mapreduce.job.acl-modify-job before allowing modifications to jobs, like:

killing a job
killing/failing a task of a job
setting the priority of a job
These operations are also permitted by the queue level ACL, "mapred.queue.queue-name.acl-administer-jobs", configured via mapred-queue-acls.xml. The caller will be able to do the operation if he/she is part of either queue admins ACL or job modification ACL.

The format of a job level ACL is the same as the format for a queue level ACL as defined in the Cluster Setup documentation.

Job Control
Users may need to chain MapReduce jobs to accomplish complex tasks which cannot be done via a single MapReduce job. This is fairly easy since the output of the job typically goes to distributed file-system, and the output, in turn, can be used as the input for the next job.

However, this also means that the onus on ensuring jobs are complete (success/failure) lies squarely on the clients. In such cases, the various job-control options are:

runJob(JobConf) : Submits the job and returns only after the job has completed.
submitJob(JobConf) : Only submits the job, then poll the returned handle to the RunningJob to query status and make scheduling decisions.
JobConf.setJobEndNotificationURI(String) : Sets up a notification upon job-completion, thus avoiding polling.
Job Credentials
In a secure cluster, the user is authenticated via Kerberos' kinit command. Because of scalability concerns, we don't push the client's Kerberos' tickets in MapReduce jobs. Instead, we acquire delegation tokens from each HDFS NameNode that the job will use and store them in the job as part of job submission. The delegation tokens are automatically obtained for the HDFS that holds the staging directories, where the job job files are written, and any HDFS systems referenced by FileInputFormats, FileOutputFormats, DistCp, and the distributed cache. Other applications require to set the configuration "mapreduce.job.hdfs-servers" for all NameNodes that tasks might need to talk during the job execution. This is a comma separated list of file system names, such as "hdfs://nn1/,hdfs://nn2/". These tokens are passed to the JobTracker as part of the job submission as Credentials.

Similar to HDFS delegation tokens, we also have MapReduce delegation tokens. The MapReduce tokens are provided so that tasks can spawn jobs if they wish to. The tasks authenticate to the JobTracker via the MapReduce delegation tokens. The delegation token can be obtained via the API in JobClient.getDelegationToken. The obtained token must then be pushed onto the credentials that is there in the JobConf used for job submission. The API Credentials.addToken can be used for this.

The credentials are sent to the JobTracker as part of the job submission process. The JobTracker persists the tokens and secrets in its filesystem (typically HDFS) in a file within mapred.system.dir/JOBID. The TaskTracker localizes the file as part job localization. Tasks see an environment variable called HADOOP_TOKEN_FILE_LOCATION and the framework sets this to point to the localized file. In order to launch jobs from tasks or for doing any HDFS operation, tasks must set the configuration "mapreduce.job.credentials.binary" to point to this token file.

The HDFS delegation tokens passed to the JobTracker during job submission are are cancelled by the JobTracker when the job completes. This is the default behavior unless mapreduce.job.complete.cancel.delegation.tokens is set to false in the JobConf. For jobs whose tasks in turn spawns jobs, this should be set to false. Applications sharing JobConf objects between multiple jobs on the JobClient side should look at setting mapreduce.job.complete.cancel.delegation.tokens to false. This is because the Credentials object within the JobConf will then be shared. All jobs will end up sharing the same tokens, and hence the tokens should not be canceled when the jobs in the sequence finish.

Apart from the HDFS delegation tokens, arbitrary secrets can also be passed during the job submission for tasks to access other third party services. The APIs JobConf.getCredentials or JobContext.getCredentials() should be used to get the credentials object and then Credentials.addSecretKey should be used to add secrets.

For applications written using the old MapReduce API, the Mapper/Reducer classes need to implement JobConfigurable in order to get access to the credentials in the tasks. A reference to the JobConf passed in the JobConfigurable.configure should be stored. In the new MapReduce API, a similar thing can be done in the Mapper.setup method. The api JobConf.getCredentials() or the api JobContext.getCredentials() should be used to get the credentials reference (depending on whether the new MapReduce API or the old MapReduce API is used). Tasks can access the secrets using the APIs in Credentials

Job Input
InputFormat describes the input-specification for a MapReduce job.

The MapReduce framework relies on the InputFormat of the job to:

Validate the input-specification of the job.
Split-up the input file(s) into logical InputSplit instances, each of which is then assigned to an individual Mapper.
Provide the RecordReader implementation used to glean input records from the logical InputSplit for processing by the Mapper.
The default behavior of file-based InputFormat implementations, typically sub-classes of FileInputFormat, is to split the input into logical InputSplit instances based on the total size, in bytes, of the input files. However, the FileSystem blocksize of the input files is treated as an upper bound for input splits. A lower bound on the split size can be set via mapred.min.split.size.

Clearly, logical splits based on input-size is insufficient for many applications since record boundaries must be respected. In such cases, the application should implement a RecordReader, who is responsible for respecting record-boundaries and presents a record-oriented view of the logical InputSplit to the individual task.

TextInputFormat is the default InputFormat.

If TextInputFormat is the InputFormat for a given job, the framework detects input-files with the .gz extensions and automatically decompresses them using the appropriate CompressionCodec. However, it must be noted that compressed files with the above extensions cannot be split and each compressed file is processed in its entirety by a single mapper.

InputSplit
InputSplit represents the data to be processed by an individual Mapper.

Typically InputSplit presents a byte-oriented view of the input, and it is the responsibility of RecordReader to process and present a record-oriented view.

FileSplit is the default InputSplit. It sets map.input.file to the path of the input file for the logical split.

RecordReader
RecordReader reads <key, value> pairs from an InputSplit.

Typically the RecordReader converts the byte-oriented view of the input, provided by the InputSplit, and presents a record-oriented to the Mapper implementations for processing. RecordReader thus assumes the responsibility of processing record boundaries and presents the tasks with keys and values.

Job Output
OutputFormat describes the output-specification for a MapReduce job.

The MapReduce framework relies on the OutputFormat of the job to:

Validate the output-specification of the job; for example, check that the output directory doesn't already exist.
Provide the RecordWriter implementation used to write the output files of the job. Output files are stored in a FileSystem.
TextOutputFormat is the default OutputFormat.

OutputCommitter
OutputCommitter describes the commit of task output for a MapReduce job.

The MapReduce framework relies on the OutputCommitter of the job to:

Setup the job during initialization. For example, create the temporary output directory for the job during the initialization of the job. Job setup is done by a separate task when the job is in PREP state and after initializing tasks. Once the setup task completes, the job will be moved to RUNNING state.
Cleanup the job after the job completion. For example, remove the temporary output directory after the job completion. Job cleanup is done by a separate task at the end of the job. Job is declared SUCCEDED/FAILED/KILLED after the cleanup task completes.
Setup the task temporary output. Task setup is done as part of the same task, during task initialization.
Check whether a task needs a commit. This is to avoid the commit procedure if a task does not need commit.
Commit of the task output. Once task is done, the task will commit it's output if required.
Discard the task commit. If the task has been failed/killed, the output will be cleaned-up. If task could not cleanup (in exception block), a separate task will be launched with same attempt-id to do the cleanup.
FileOutputCommitter is the default OutputCommitter. Job setup/cleanup tasks occupy map or reduce slots, whichever is free on the TaskTracker. And JobCleanup task, TaskCleanup tasks and JobSetup task have the highest priority, and in that order.

Task Side-Effect Files
In some applications, component tasks need to create and/or write to side-files, which differ from the actual job-output files.

In such cases there could be issues with two instances of the same Mapper or Reducer running simultaneously (for example, speculative tasks) trying to open and/or write to the same file (path) on the FileSystem. Hence the application-writer will have to pick unique names per task-attempt (using the attemptid, say attempt_200709221812_0001_m_000000_0), not just per task.

To avoid these issues the MapReduce framework, when the OutputCommitter is FileOutputCommitter, maintains a special ${mapred.output.dir}/_temporary/_${taskid} sub-directory accessible via ${mapred.work.output.dir} for each task-attempt on the FileSystem where the output of the task-attempt is stored. On successful completion of the task-attempt, the files in the ${mapred.output.dir}/_temporary/_${taskid} (only) are promoted to ${mapred.output.dir}. Of course, the framework discards the sub-directory of unsuccessful task-attempts. This process is completely transparent to the application.

The application-writer can take advantage of this feature by creating any side-files required in ${mapred.work.output.dir} during execution of a task via FileOutputFormat.getWorkOutputPath(), and the framework will promote them similarly for succesful task-attempts, thus eliminating the need to pick unique paths per task-attempt.

Note: The value of ${mapred.work.output.dir} during execution of a particular task-attempt is actually ${mapred.output.dir}/_temporary/_{$taskid}, and this value is set by the MapReduce framework. So, just create any side-files in the path returned by FileOutputFormat.getWorkOutputPath() from MapReduce task to take advantage of this feature.

The entire discussion holds true for maps of jobs with reducer=NONE (i.e. 0 reduces) since output of the map, in that case, goes directly to HDFS.

RecordWriter
RecordWriter writes the output <key, value> pairs to an output file.

RecordWriter implementations write the job outputs to the FileSystem.

Other Useful Features
Submitting Jobs to Queues
Users submit jobs to Queues. Queues, as collection of jobs, allow the system to provide specific functionality. For example, queues use ACLs to control which users who can submit jobs to them. Queues are expected to be primarily used by Hadoop Schedulers.

Hadoop comes configured with a single mandatory queue, called 'default'. Queue names are defined in the mapred.queue.names property of the Hadoop site configuration. Some job schedulers, such as the Capacity Scheduler, support multiple queues.

A job defines the queue it needs to be submitted to through the mapred.job.queue.name property, or through the setQueueName(String) API. Setting the queue name is optional. If a job is submitted without an associated queue name, it is submitted to the 'default' queue.

Counters
Counters represent global counters, defined either by the MapReduce framework or applications. Each Counter can be of any Enum type. Counters of a particular Enum are bunched into groups of type Counters.Group.

Applications can define arbitrary Counters (of type Enum) and update them via Reporter.incrCounter(Enum, long) or Reporter.incrCounter(String, String, long) in the map and/or reduce methods. These counters are then globally aggregated by the framework.

DistributedCache
DistributedCache distributes application-specific, large, read-only files efficiently.

DistributedCache is a facility provided by the MapReduce framework to cache files (text, archives, jars and so on) needed by applications.

Applications specify the files to be cached via urls (hdfs://) in the JobConf. The DistributedCache assumes that the files specified via hdfs:// urls are already present on the FileSystem.

The framework will copy the necessary files to the slave node before any tasks for the job are executed on that node. Its efficiency stems from the fact that the files are only copied once per job and the ability to cache archives which are un-archived on the slaves.

DistributedCache tracks the modification timestamps of the cached files. Clearly the cache files should not be modified by the application or externally while the job is executing.

DistributedCache can be used to distribute simple, read-only data/text files and more complex types such as archives and jars. Archives (zip, tar, tgz and tar.gz files) are un-archived at the slave nodes. Files have execution permissions set.

The files/archives can be distributed by setting the property mapred.cache.{files|archives}. If more than one file/archive has to be distributed, they can be added as comma separated paths. The properties can also be set by APIs DistributedCache.addCacheFile(URI,conf)/ DistributedCache.addCacheArchive(URI,conf) and DistributedCache.setCacheFiles(URIs,conf)/ DistributedCache.setCacheArchives(URIs,conf) where URI is of the form hdfs://host:port/absolute-path#link-name. In Streaming, the files can be distributed through command line option -cacheFile/-cacheArchive.

Optionally users can also direct the DistributedCache to symlink the cached file(s) into the current working directory of the task via the DistributedCache.createSymlink(Configuration) api. Or by setting the configuration property mapred.create.symlink as yes. The DistributedCache will use the fragment of the URI as the name of the symlink. For example, the URI hdfs://namenode:port/lib.so.1#lib.so will have the symlink name as lib.so in task's cwd for the file lib.so.1 in distributed cache.

The DistributedCache can also be used as a rudimentary software distribution mechanism for use in the map and/or reduce tasks. It can be used to distribute both jars and native libraries. The DistributedCache.addArchiveToClassPath(Path, Configuration) or DistributedCache.addFileToClassPath(Path, Configuration) api can be used to cache files/jars and also add them to the classpath of child-jvm. The same can be done by setting the configuration properties mapred.job.classpath.{files|archives}. Similarly the cached files that are symlinked into the working directory of the task can be used to distribute native libraries and load them.

Private and Public DistributedCache Files
DistributedCache files can be private or public, that determines how they can be shared on the slave nodes.

"Private" DistributedCache files are cached in a local directory private to the user whose jobs need these files. These files are shared by all tasks and jobs of the specific user only and cannot be accessed by jobs of other users on the slaves. A DistributedCache file becomes private by virtue of its permissions on the file system where the files are uploaded, typically HDFS. If the file has no world readable access, or if the directory path leading to the file has no world executable access for lookup, then the file becomes private.
"Public" DistributedCache files are cached in a global directory and the file access is setup such that they are publicly visible to all users. These files can be shared by tasks and jobs of all users on the slaves. A DistributedCache file becomes public by virtue of its permissions on the file system where the files are uploaded, typically HDFS. If the file has world readable access, AND if the directory path leading to the file has world executable access for lookup, then the file becomes public. In other words, if the user intends to make a file publicly available to all users, the file permissions must be set to be world readable, and the directory permissions on the path leading to the file must be world executable.
Tool
The Tool interface supports the handling of generic Hadoop command-line options.

Tool is the standard for any MapReduce tool or application. The application should delegate the handling of standard command-line options to GenericOptionsParser via ToolRunner.run(Tool, String[]) and only handle its custom arguments.

The generic Hadoop command-line options are:
-conf <configuration file> 
-D <property=value> 
-fs <local|namenode:port> 
-jt <local|jobtracker:port>

IsolationRunner
IsolationRunner is a utility to help debug MapReduce programs.

To use the IsolationRunner, first set keep.failed.task.files to true (also see keep.task.files.pattern).

Next, go to the node on which the failed task ran and go to the TaskTracker's local directory and run the IsolationRunner:
$ cd <local path>/taskTracker/${taskid}/work 
$ bin/hadoop org.apache.hadoop.mapred.IsolationRunner ../job.xml

IsolationRunner will run the failed task in a single jvm, which can be in the debugger, over precisely the same input.

Note that currently IsolationRunner will only re-run map tasks.

Profiling
Profiling is a utility to get a representative (2 or 3) sample of built-in java profiler for a sample of maps and reduces.

User can specify whether the system should collect profiler information for some of the tasks in the job by setting the configuration property mapred.task.profile. The value can be set using the api JobConf.setProfileEnabled(boolean). If the value is set true, the task profiling is enabled. The profiler information is stored in the user log directory. By default, profiling is not enabled for the job.

Once user configures that profiling is needed, she/he can use the configuration property mapred.task.profile.{maps|reduces} to set the ranges of MapReduce tasks to profile. The value can be set using the api JobConf.setProfileTaskRange(boolean,String). By default, the specified range is 0-2.

User can also specify the profiler configuration arguments by setting the configuration property mapred.task.profile.params. The value can be specified using the api JobConf.setProfileParams(String). If the string contains a %s, it will be replaced with the name of the profiling output file when the task runs. These parameters are passed to the task child JVM on the command line. The default value for the profiling parameters is -agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s

Debugging
The MapReduce framework provides a facility to run user-provided scripts for debugging. When a MapReduce task fails, a user can run a debug script, to process task logs for example. The script is given access to the task's stdout and stderr outputs, syslog and jobconf. The output from the debug script's stdout and stderr is displayed on the console diagnostics and also as part of the job UI.

In the following sections we discuss how to submit a debug script with a job. The script file needs to be distributed and submitted to the framework.

How to distribute the script file:
The user needs to use DistributedCache to distribute and symlink the script file.

How to submit the script:
A quick way to submit the debug script is to set values for the properties mapred.map.task.debug.script and mapred.reduce.task.debug.script, for debugging map and reduce tasks respectively. These properties can also be set by using APIs JobConf.setMapDebugScript(String) and JobConf.setReduceDebugScript(String) . In streaming mode, a debug script can be submitted with the command-line options -mapdebug and -reducedebug, for debugging map and reduce tasks respectively.

The arguments to the script are the task's stdout, stderr, syslog and jobconf files. The debug command, run on the node where the MapReduce task failed, is: 
$script $stdout $stderr $syslog $jobconf

Pipes programs have the c++ program name as a fifth argument for the command. Thus for the pipes programs the command is 
$script $stdout $stderr $syslog $jobconf $program

Default Behavior:
For pipes, a default script is run to process core dumps under gdb, prints stack trace and gives info about running threads.

JobControl
JobControl is a utility which encapsulates a set of MapReduce jobs and their dependencies.

Data Compression
Hadoop MapReduce provides facilities for the application-writer to specify compression for both intermediate map-outputs and the job-outputs i.e. output of the reduces. It also comes bundled with CompressionCodec implementation for the zlib compression algorithm. The gzip file format is also supported.

Hadoop also provides native implementations of the above compression codecs for reasons of both performance (zlib) and non-availability of Java libraries. More details on their usage and availability are available here.

Intermediate Outputs
Applications can control compression of intermediate map-outputs via the JobConf.setCompressMapOutput(boolean) api and the CompressionCodec to be used via the JobConf.setMapOutputCompressorClass(Class) api.

Job Outputs
Applications can control compression of job-outputs via the FileOutputFormat.setCompressOutput(JobConf, boolean) api and the CompressionCodec to be used can be specified via the FileOutputFormat.setOutputCompressorClass(JobConf, Class) api.

If the job outputs are to be stored in the SequenceFileOutputFormat, the required SequenceFile.CompressionType (i.e. RECORD / BLOCK - defaults to RECORD) can be specified via the SequenceFileOutputFormat.setOutputCompressionType(JobConf, SequenceFile.CompressionType) api.

Skipping Bad Records
Hadoop provides an option where a certain set of bad input records can be skipped when processing map inputs. Applications can control this feature through the SkipBadRecords class.

This feature can be used when map tasks crash deterministically on certain input. This usually happens due to bugs in the map function. Usually, the user would have to fix these bugs. This is, however, not possible sometimes. The bug may be in third party libraries, for example, for which the source code is not available. In such cases, the task never completes successfully even after multiple attempts, and the job fails. With this feature, only a small portion of data surrounding the bad records is lost, which may be acceptable for some applications (those performing statistical analysis on very large data, for example).

By default this feature is disabled. For enabling it, refer to SkipBadRecords.setMapperMaxSkipRecords(Configuration, long) and SkipBadRecords.setReducerMaxSkipGroups(Configuration, long).

With this feature enabled, the framework gets into 'skipping mode' after a certain number of map failures. For more details, see SkipBadRecords.setAttemptsToStartSkipping(Configuration, int). In 'skipping mode', map tasks maintain the range of records being processed. To do this, the framework relies on the processed record counter. See SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS and SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS. This counter enables the framework to know how many records have been processed successfully, and hence, what record range caused a task to crash. On further attempts, this range of records is skipped.

The number of records skipped depends on how frequently the processed record counter is incremented by the application. It is recommended that this counter be incremented after every record is processed. This may not be possible in some applications that typically batch their processing. In such cases, the framework may skip additional records surrounding the bad record. Users can control the number of skipped records through SkipBadRecords.setMapperMaxSkipRecords(Configuration, long) and SkipBadRecords.setReducerMaxSkipGroups(Configuration, long). The framework tries to narrow the range of skipped records using a binary search-like approach. The skipped range is divided into two halves and only one half gets executed. On subsequent failures, the framework figures out which half contains bad records. A task will be re-executed till the acceptable skipped value is met or all task attempts are exhausted. To increase the number of task attempts, use JobConf.setMaxMapAttempts(int) and JobConf.setMaxReduceAttempts(int).

Skipped records are written to HDFS in the sequence file format, for later analysis. The location can be changed through SkipBadRecords.setSkipOutputPath(JobConf, Path).

Example: WordCount v2.0
Here is a more complete WordCount which uses many of the features provided by the MapReduce framework we discussed so far.

This needs the HDFS to be up and running, especially for the DistributedCache-related features. Hence it only works with a pseudo-distributed or fully-distributed Hadoop installation.

Source Code
WordCount.java
1.	package org.myorg;
2.	
3.	import java.io.*;
4.	import java.util.*;
5.	
6.	import org.apache.hadoop.fs.Path;
7.	import org.apache.hadoop.filecache.DistributedCache;
8.	import org.apache.hadoop.conf.*;
9.	import org.apache.hadoop.io.*;
10.	import org.apache.hadoop.mapred.*;
11.	import org.apache.hadoop.util.*;
12.	
13.	public class WordCount extends Configured implements Tool {
14.	
15.	    public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {
16.	
17.	      static enum Counters { INPUT_WORDS }
18.	
19.	      private final static IntWritable one = new IntWritable(1);
20.	      private Text word = new Text();
21.	
22.	      private boolean caseSensitive = true;
23.	      private Set<String> patternsToSkip = new HashSet<String>();
24.	
25.	      private long numRecords = 0;
26.	      private String inputFile;
27.	
28.	      public void configure(JobConf job) {
29.	        caseSensitive = job.getBoolean("wordcount.case.sensitive", true);
30.	        inputFile = job.get("map.input.file");
31.	
32.	        if (job.getBoolean("wordcount.skip.patterns", false)) {
33.	          Path[] patternsFiles = new Path[0];
34.	          try {
35.	            patternsFiles = DistributedCache.getLocalCacheFiles(job);
36.	          } catch (IOException ioe) {
37.	            System.err.println("Caught exception while getting cached files: " + StringUtils.stringifyException(ioe));
38.	          }
39.	          for (Path patternsFile : patternsFiles) {
40.	            parseSkipFile(patternsFile);
41.	          }
42.	        }
43.	      }
44.	
45.	      private void parseSkipFile(Path patternsFile) {
46.	        try {
47.	          BufferedReader fis = new BufferedReader(new FileReader(patternsFile.toString()));
48.	          String pattern = null;
49.	          while ((pattern = fis.readLine()) != null) {
50.	            patternsToSkip.add(pattern);
51.	          }
52.	        } catch (IOException ioe) {
53.	          System.err.println("Caught exception while parsing the cached file '" + patternsFile + "' : " + StringUtils.stringifyException(ioe));
54.	        }
55.	      }
56.	
57.	      public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {
58.	        String line = (caseSensitive) ? value.toString() : value.toString().toLowerCase();
59.	
60.	        for (String pattern : patternsToSkip) {
61.	          line = line.replaceAll(pattern, "");
62.	        }
63.	
64.	        StringTokenizer tokenizer = new StringTokenizer(line);
65.	        while (tokenizer.hasMoreTokens()) {
66.	          word.set(tokenizer.nextToken());
67.	          output.collect(word, one);
68.	          reporter.incrCounter(Counters.INPUT_WORDS, 1);
69.	        }
70.	
71.	        if ((++numRecords % 100) == 0) {
72.	          reporter.setStatus("Finished processing " + numRecords + " records " + "from the input file: " + inputFile);
73.	        }
74.	      }
75.	    }
76.	
77.	    public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {
78.	      public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {
79.	        int sum = 0;
80.	        while (values.hasNext()) {
81.	          sum += values.next().get();
82.	        }
83.	        output.collect(key, new IntWritable(sum));
84.	      }
85.	    }
86.	
87.	    public int run(String[] args) throws Exception {
88.	      JobConf conf = new JobConf(getConf(), WordCount.class);
89.	      conf.setJobName("wordcount");
90.	
91.	      conf.setOutputKeyClass(Text.class);
92.	      conf.setOutputValueClass(IntWritable.class);
93.	
94.	      conf.setMapperClass(Map.class);
95.	      conf.setCombinerClass(Reduce.class);
96.	      conf.setReducerClass(Reduce.class);
97.	
98.	      conf.setInputFormat(TextInputFormat.class);
99.	      conf.setOutputFormat(TextOutputFormat.class);
100.	
101.	      List<String> other_args = new ArrayList<String>();
102.	      for (int i=0; i < args.length; ++i) {
103.	        if ("-skip".equals(args[i])) {
104.	          DistributedCache.addCacheFile(new Path(args[++i]).toUri(), conf);
105.	          conf.setBoolean("wordcount.skip.patterns", true);
106.	        } else {
107.	          other_args.add(args[i]);
108.	        }
109.	      }
110.	
111.	      FileInputFormat.setInputPaths(conf, new Path(other_args.get(0)));
112.	      FileOutputFormat.setOutputPath(conf, new Path(other_args.get(1)));
113.	
114.	      JobClient.runJob(conf);
115.	      return 0;
116.	    }
117.	
118.	    public static void main(String[] args) throws Exception {
119.	      int res = ToolRunner.run(new Configuration(), new WordCount(), args);
120.	      System.exit(res);
121.	    }
122.	}
123.	
Sample Runs
Sample text-files as input:

$ bin/hadoop dfs -ls /usr/joe/wordcount/input/ 
/usr/joe/wordcount/input/file01 
/usr/joe/wordcount/input/file02 

$ bin/hadoop dfs -cat /usr/joe/wordcount/input/file01 
Hello World, Bye World! 

$ bin/hadoop dfs -cat /usr/joe/wordcount/input/file02 
Hello Hadoop, Goodbye to hadoop.

Run the application:

$ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount /usr/joe/wordcount/input /usr/joe/wordcount/output

Output:

$ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 
Bye 1 
Goodbye 1 
Hadoop, 1 
Hello 2 
World! 1 
World, 1 
hadoop. 1 
to 1 
Notice that the inputs differ from the first version we looked at, and how they affect the outputs.

Now, lets plug-in a pattern-file which lists the word-patterns to be ignored, via the DistributedCache.

$ hadoop dfs -cat /user/joe/wordcount/patterns.txt 
\. 
\, 
\! 
to 
Run it again, this time with more options:

$ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount -Dwordcount.case.sensitive=true /usr/joe/wordcount/input /usr/joe/wordcount/output -skip /user/joe/wordcount/patterns.txt

As expected, the output:

$ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 
Bye 1 
Goodbye 1 
Hadoop 1 
Hello 2 
World 2 
hadoop 1 
Run it once more, this time switch-off case-sensitivity:

$ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount -Dwordcount.case.sensitive=false /usr/joe/wordcount/input /usr/joe/wordcount/output -skip /user/joe/wordcount/patterns.txt

Sure enough, the output:

$ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 
bye 1 
goodbye 1 
hadoop 2 
hello 2 
world 2 
Highlights
The second version of WordCount improves upon the previous one by using some features offered by the MapReduce framework:

Demonstrates how applications can access configuration parameters in the configure method of the Mapper (and Reducer) implementations (lines 28-43).
Demonstrates how the DistributedCache can be used to distribute read-only data needed by the jobs. Here it allows the user to specify word-patterns to skip while counting (line 104).
Demonstrates the utility of the Tool interface and the GenericOptionsParser to handle generic Hadoop command-line options (lines 87-116, 119).
Demonstrates how applications can use Counters (line 68) and how they can set application-specific status information via the Reporter instance passed to the map (and reduce) method (line 72).
Java and JNI are trademarks or registered trademarks of Sun Microsystems, Inc. in the United States and other countries.

 
Last Published: 08/04/2013 08:13:21
Copyright © 2008 The Apache Software Foundation.
Apache > Hadoop > Core > docs > r1.2.1
Hadoop Hadoop
   
ProjectWikiHadoop 1.2.1 Documentation
Last Published: 08/04/2013 08:13:21
 
Getting Started
Guides
MapReduce
HDFS
HDFS Users
HDFS Architecture
Permissions
Quotas
Synthetic Load Generator
Offline Image Viewer
HFTP
WebHDFS REST API
C API libhdfs
Common
Miscellaneous

PDF -icon
PDF
HDFS Users Guide
Purpose
Overview
Prerequisites
Web Interface
Shell Commands
DFSAdmin Command
Secondary NameNode
Import Checkpoint
Rebalancer
Rack Awareness
Safemode
fsck
fetchdt
Recovery Mode
Upgrade and Rollback
File Permissions and Security
Scalability
Related Documentation
Purpose
This document is a starting point for users working with Hadoop Distributed File System (HDFS) either as a part of a Hadoop cluster or as a stand-alone general purpose distributed file system. While HDFS is designed to "just work" in many environments, a working knowledge of HDFS helps greatly with configuration improvements and diagnostics on a specific cluster.

Overview
HDFS is the primary distributed storage used by Hadoop applications. A HDFS cluster primarily consists of a NameNode that manages the file system metadata and DataNodes that store the actual data. The HDFS Architecture Guide describes HDFS in detail. This user guide primarily deals with the interaction of users and administrators with HDFS clusters. The HDFS architecture diagram depicts basic interactions among NameNode, the DataNodes, and the clients. Clients contact NameNode for file metadata or file modifications and perform actual file I/O directly with the DataNodes.

The following are some of the salient features that could be of interest to many users.

Hadoop, including HDFS, is well suited for distributed storage and distributed processing using commodity hardware. It is fault tolerant, scalable, and extremely simple to expand. MapReduce, well known for its simplicity and applicability for large set of distributed applications, is an integral part of Hadoop.
HDFS is highly configurable with a default configuration well suited for many installations. Most of the time, configuration needs to be tuned only for very large clusters.
Hadoop is written in Java and is supported on all major platforms.
Hadoop supports shell-like commands to interact with HDFS directly.
The NameNode and Datanodes have built in web servers that makes it easy to check current status of the cluster.
New features and improvements are regularly implemented in HDFS. The following is a subset of useful features in HDFS:
File permissions and authentication.
Rack awareness: to take a node's physical location into account while scheduling tasks and allocating storage.
Safemode: an administrative mode for maintenance.
fsck: a utility to diagnose health of the file system, to find missing files or blocks.
fetchdt: a utility to fetch DelegationToken and store it in a file on the local system.
Rebalancer: tool to balance the cluster when the data is unevenly distributed among DataNodes.
Upgrade and rollback: after a software upgrade, it is possible to rollback to HDFS' state before the upgrade in case of unexpected problems.
Secondary NameNode: performs periodic checkpoints of the namespace and helps keep the size of file containing log of HDFS modifications within certain limits at the NameNode.
Prerequisites
The following documents describe how to install and set up a Hadoop cluster:

Single Node Setup for first-time users.
Cluster Setup for large, distributed clusters.
The rest of this document assumes the user is able to set up and run a HDFS with at least one DataNode. For the purpose of this document, both the NameNode and DataNode could be running on the same physical machine.

Web Interface
NameNode and DataNode each run an internal web server in order to display basic information about the current status of the cluster. With the default configuration, the NameNode front page is at http://namenode-name:50070/. It lists the DataNodes in the cluster and basic statistics of the cluster. The web interface can also be used to browse the file system (using "Browse the file system" link on the NameNode front page).

Shell Commands
Hadoop includes various shell-like commands that directly interact with HDFS and other file systems that Hadoop supports. The command bin/hdfs dfs -help lists the commands supported by Hadoop shell. Furthermore, the command bin/hdfs dfs -help command-name displays more detailed help for a command. These commands support most of the normal files system operations like copying files, changing file permissions, etc. It also supports a few HDFS specific operations like changing replication of files. For more information see File System Shell Guide.

DFSAdmin Command
The bin/hadoop dfsadmin command supports a few HDFS administration related operations. The bin/hadoop dfsadmin -help command lists all the commands currently supported. For e.g.:

-report : reports basic statistics of HDFS. Some of this information is also available on the NameNode front page.
-safemode : though usually not required, an administrator can manually enter or leave Safemode.
-finalizeUpgrade : removes previous backup of the cluster made during last upgrade.
-refreshNodes : Updates the set of hosts allowed to connect to namenode. Re-reads the config file to update values defined by dfs.hosts and dfs.host.exclude and reads the entires (hostnames) in those files. Each entry not defined in dfs.hosts but in dfs.hosts.exclude is decommissioned. Each entry defined in dfs.hosts and also in dfs.host.exclude is stopped from decommissioning if it has aleady been marked for decommission. Entires not present in both the lists are decommissioned.
For command usage, see dfsadmin.

Secondary NameNode
The NameNode stores modifications to the file system as a log appended to a native file system file, edits. When a NameNode starts up, it reads HDFS state from an image file, fsimage, and then applies edits from the edits log file. It then writes new HDFS state to the fsimage and starts normal operation with an empty edits file. Since NameNode merges fsimage and edits files only during start up, the edits log file could get very large over time on a busy cluster. Another side effect of a larger edits file is that next restart of NameNode takes longer.

The secondary NameNode merges the fsimage and the edits log files periodically and keeps edits log size within a limit. It is usually run on a different machine than the primary NameNode since its memory requirements are on the same order as the primary NameNode. The secondary NameNode is started by bin/start-dfs.sh on the nodes specified in conf/masters file.

The start of the checkpoint process on the secondary NameNode is controlled by two configuration parameters.

fs.checkpoint.period, set to 1 hour by default, specifies the maximum delay between two consecutive checkpoints, and
fs.checkpoint.size, set to 64MB by default, defines the size of the edits log file that forces an urgent checkpoint even if the maximum checkpoint delay is not reached.
The secondary NameNode stores the latest checkpoint in a directory which is structured the same way as the primary NameNode's directory. So that the check pointed image is always ready to be read by the primary NameNode if necessary.

For command usage, see secondarynamenode.

Import Checkpoint
The latest checkpoint can be imported to the NameNode if all other copies of the image and the edits files are lost. In order to do that one should:

Create an empty directory specified in the dfs.name.dir configuration variable;
Specify the location of the checkpoint directory in the configuration variable fs.checkpoint.dir;
and start the NameNode with -importCheckpoint option.
The NameNode will upload the checkpoint from the fs.checkpoint.dir directory and then save it to the NameNode directory(s) set in dfs.name.dir. The NameNode will fail if a legal image is contained in dfs.name.dir. The NameNode verifies that the image in fs.checkpoint.dir is consistent, but does not modify it in any way.

For command usage, see namenode.

Rebalancer
HDFS data might not always be be placed uniformly across the DataNode. One common reason is addition of new DataNodes to an existing cluster. While placing new blocks (data for a file is stored as a series of blocks), NameNode considers various parameters before choosing the DataNodes to receive these blocks. Some of the considerations are:

Policy to keep one of the replicas of a block on the same node as the node that is writing the block.
Need to spread different replicas of a block across the racks so that cluster can survive loss of whole rack.
One of the replicas is usually placed on the same rack as the node writing to the file so that cross-rack network I/O is reduced.
Spread HDFS data uniformly across the DataNodes in the cluster.
Due to multiple competing considerations, data might not be uniformly placed across the DataNodes. HDFS provides a tool for administrators that analyzes block placement and rebalanaces data across the DataNode. A brief administrator's guide for rebalancer as a PDF is attached to HADOOP-1652.

For command usage, see balancer.

Rack Awareness
Typically large Hadoop clusters are arranged in racks and network traffic between different nodes with in the same rack is much more desirable than network traffic across the racks. In addition NameNode tries to place replicas of block on multiple racks for improved fault tolerance. Hadoop lets the cluster administrators decide which rack a node belongs to through configuration variable dfs.network.script. When this script is configured, each node runs the script to determine its rack id. A default installation assumes all the nodes belong to the same rack. This feature and configuration is further described in PDF attached to HADOOP-692.

Safemode
During start up the NameNode loads the file system state from the fsimage and the edits log file. It then waits for DataNodes to report their blocks so that it does not prematurely start replicating the blocks though enough replicas already exist in the cluster. During this time NameNode stays in Safemode. Safemode for the NameNode is essentially a read-only mode for the HDFS cluster, where it does not allow any modifications to file system or blocks. Normally the NameNode leaves Safemode automatically after the DataNodes have reported that most file system blocks are available. If required, HDFS could be placed in Safemode explicitly using 'bin/hadoop dfsadmin -safemode' command. NameNode front page shows whether Safemode is on or off. A more detailed description and configuration is maintained as JavaDoc for NameNode.setSafeMode().

fsck
HDFS supports the fsck command to check for various inconsistencies. It it is designed for reporting problems with various files, for example, missing blocks for a file or under-replicated blocks. Unlike a traditional fsck utility for native file systems, this command does not correct the errors it detects. Normally NameNode automatically corrects most of the recoverable failures. By default fsck ignores open files but provides an option to select all files during reporting. The HDFS fsck command is not a Hadoop shell command. It can be run as 'bin/hadoop fsck'. For command usage, see fsck. fsck can be run on the whole file system or on a subset of files.

fetchdt
HDFS supports the fetchdt command to fetch Delegation Token and store it in a file on the local system. This token can be later used to access secure server (NameNode for example) from a non secure client. Utility uses either RPC or HTTPS (over Kerberos) to get the token, and thus requires kerberos tickets to be present before the run (run kinit to get the tickets). The HDFS fetchdt command is not a Hadoop shell command. It can be run as 'bin/hadoop fetchdt DTfile '. After you got the token you can run an HDFS command without having Kerberos tickets, by pointing HADOOP_TOKEN_FILE_LOCATION environmental variable to the delegation token file. For command usage, see fetchdt command.

Recovery Mode
Typically, you will configure multiple metadata storage locations. Then, if one storage location is corrupt, you can read the metadata from one of the other storage locations.

However, what can you do if the only storage locations available are corrupt? In this case, there is a special NameNode startup mode called Recovery mode that may allow you to recover most of your data.

You can start the NameNode in recovery mode like so: namenode -recover

When in recovery mode, the NameNode will interactively prompt you at the command line about possible courses of action you can take to recover your data.

If you don't want to be prompted, you can give the -force option. This option will force recovery mode to always select the first choice. Normally, this will be the most reasonable choice.

Because Recovery mode can cause you to lose data, you should always back up your edit log and fsimage before using it.

Upgrade and Rollback
When Hadoop is upgraded on an existing cluster, as with any software upgrade, it is possible there are new bugs or incompatible changes that affect existing applications and were not discovered earlier. In any non-trivial HDFS installation, it is not an option to loose any data, let alone to restart HDFS from scratch. HDFS allows administrators to go back to earlier version of Hadoop and rollback the cluster to the state it was in before the upgrade. HDFS upgrade is described in more detail in Hadoop Upgrade Wiki page. HDFS can have one such backup at a time. Before upgrading, administrators need to remove existing backup using bin/hadoop dfsadmin -finalizeUpgrade command. The following briefly describes the typical upgrade procedure:

Before upgrading Hadoop software, finalize if there an existing backup. dfsadmin -upgradeProgress status can tell if the cluster needs to be finalized.
Stop the cluster and distribute new version of Hadoop.
Run the new version with -upgrade option (bin/start-dfs.sh -upgrade).
Most of the time, cluster works just fine. Once the new HDFS is considered working well (may be after a few days of operation), finalize the upgrade. Note that until the cluster is finalized, deleting the files that existed before the upgrade does not free up real disk space on the DataNodes.
If there is a need to move back to the old version,
stop the cluster and distribute earlier version of Hadoop.
start the cluster with rollback option. (bin/start-dfs.h -rollback).
File Permissions and Security
The file permissions are designed to be similar to file permissions on other familiar platforms like Linux. Currently, security is limited to simple file permissions. The user that starts NameNode is treated as the superuser for HDFS. Future versions of HDFS will support network authentication protocols like Kerberos for user authentication and encryption of data transfers. The details are discussed in the Permissions Guide.

Scalability
Hadoop currently runs on clusters with thousands of nodes. The PoweredBy Wiki page lists some of the organizations that deploy Hadoop on large clusters. HDFS has one NameNode for each cluster. Currently the total memory available on NameNode is the primary scalability limitation. On very large clusters, increasing average size of files stored in HDFS helps with increasing cluster size without increasing memory requirements on NameNode. The default configuration may not suite very large clustes. The FAQ Wiki page lists suggested configuration improvements for large Hadoop clusters.

Related Documentation
This user guide is a good starting point for working with HDFS. While the user guide continues to improve, there is a large wealth of documentation about Hadoop and HDFS. The following list is a starting point for further exploration:

Hadoop Site: The home page for the Apache Hadoop site.
Hadoop Wiki: The home page (FrontPage) for the Hadoop Wiki. Unlike the released documentation, which is part of Hadoop source tree, Hadoop Wiki is regularly edited by Hadoop Community.
FAQ: The FAQ Wiki page.
Hadoop JavaDoc API.
Hadoop User Mailing List : core-user[at]hadoop.apache.org.
Explore src/hdfs/hdfs-default.xml. It includes brief description of most of the configuration variables available.
Hadoop Commands Guide: Hadoop commands usage.
 
Last Published: 08/04/2013 08:13:21
Copyright © 2008 The Apache Software Foundation.
Apache > Hadoop > Core > docs > r1.2.1
Hadoop Hadoop
   
ProjectWikiHadoop 1.2.1 Documentation
Last Published: 08/04/2013 08:13:21
 
Getting Started
Guides
MapReduce
HDFS
Common
Deployment Layout
File System Shell
Service Level Authorization
Native Libraries
Miscellaneous

PDF -icon
PDF
Service Level Authorization Guide
Purpose
Prerequisites
Overview
Configuration
Enable Service Level Authorization
Hadoop Services and Configuration Properties
Access Control Lists
Refreshing Service Level Authorization Configuration
Examples
Purpose
This document describes how to configure and manage Service Level Authorization for Hadoop.

Prerequisites
Make sure Hadoop is installed, configured and setup correctly. For more information see:

Single Node Setup for first-time users.
Cluster Setup for large, distributed clusters.
Overview
Service Level Authorization is the initial authorization mechanism to ensure clients connecting to a particular Hadoop service have the necessary, pre-configured, permissions and are authorized to access the given service. For example, a MapReduce cluster can use this mechanism to allow a configured list of users/groups to submit jobs.

The ${HADOOP_CONF_DIR}/hadoop-policy.xml configuration file is used to define the access control lists for various Hadoop services.

Service Level Authorization is performed much before to other access control checks such as file-permission checks, access control on job queues etc.

Configuration
This section describes how to configure service-level authorization via the configuration file {HADOOP_CONF_DIR}/hadoop-policy.xml.

Enable Service Level Authorization
By default, service-level authorization is disabled for Hadoop. To enable it set the configuration property hadoop.security.authorization to true in ${HADOOP_CONF_DIR}/core-site.xml.

Hadoop Services and Configuration Properties
This section lists the various Hadoop services and their configuration knobs:

Property	Service
security.client.protocol.acl	ACL for ClientProtocol, which is used by user code via the DistributedFileSystem.
security.client.datanode.protocol.acl	ACL for ClientDatanodeProtocol, the client-to-datanode protocol for block recovery.
security.datanode.protocol.acl	ACL for DatanodeProtocol, which is used by datanodes to communicate with the namenode.
security.inter.datanode.protocol.acl	ACL for InterDatanodeProtocol, the inter-datanode protocol for updating generation timestamp.
security.namenode.protocol.acl	ACL for NamenodeProtocol, the protocol used by the secondary namenode to communicate with the namenode.
security.inter.tracker.protocol.acl	ACL for InterTrackerProtocol, used by the tasktrackers to communicate with the jobtracker.
security.job.submission.protocol.acl	ACL for JobSubmissionProtocol, used by job clients to communciate with the jobtracker for job submission, querying job status etc.
security.task.umbilical.protocol.acl	ACL for TaskUmbilicalProtocol, used by the map and reduce tasks to communicate with the parent tasktracker.
security.refresh.policy.protocol.acl	ACL for RefreshAuthorizationPolicyProtocol, used by the dfsadmin and mradmin commands to refresh the security policy in-effect.
Access Control Lists
${HADOOP_CONF_DIR}/hadoop-policy.xml defines an access control list for each Hadoop service. Every access control list has a simple format:

The list of users and groups are both comma separated list of names. The two lists are separated by a space.

Example: user1,user2 group1,group2.

Add a blank at the beginning of the line if only a list of groups is to be provided, equivalently a comman-separated list of users followed by a space or nothing implies only a set of given users.

A special value of * implies that all users are allowed to access the service.

Refreshing Service Level Authorization Configuration
The service-level authorization configuration for the NameNode and JobTracker can be changed without restarting either of the Hadoop master daemons. The cluster administrator can change ${HADOOP_CONF_DIR}/hadoop-policy.xml on the master nodes and instruct the NameNode and JobTracker to reload their respective configurations via the -refreshServiceAcl switch to dfsadmin and mradmin commands respectively.

Refresh the service-level authorization configuration for the NameNode:

$ bin/hadoop dfsadmin -refreshServiceAcl

Refresh the service-level authorization configuration for the JobTracker:

$ bin/hadoop mradmin -refreshServiceAcl

Of course, one can use the security.refresh.policy.protocol.acl property in ${HADOOP_CONF_DIR}/hadoop-policy.xml to restrict access to the ability to refresh the service-level authorization configuration to certain users/groups.

Examples
Allow only users alice, bob and users in the mapreduce group to submit jobs to the MapReduce cluster:

<property>
     <name>security.job.submission.protocol.acl</name>
     <value>alice,bob mapreduce</value>
</property>
Allow only DataNodes running as the users who belong to the group datanodes to communicate with the NameNode:

<property>
     <name>security.datanode.protocol.acl</name>
     <value>datanodes</value>
</property>
Allow any user to talk to the HDFS cluster as a DFSClient:

<property>
     <name>security.client.protocol.acl</name>
     <value>*</value>
</property>
 
Last Published: 08/04/2013 08:13:21
Copyright © 2008 The Apache Software Foundation.
Apache > Hadoop > Core > docs > r1.2.1
Hadoop Hadoop
   
ProjectWikiHadoop 1.2.1 Documentation
Last Published: 08/04/2013 08:13:21
 
Getting Started
Guides
MapReduce
HDFS
Common
Deployment Layout
File System Shell
Service Level Authorization
Native Libraries
Miscellaneous

PDF -icon
PDF
Service Level Authorization Guide
Purpose
Prerequisites
Overview
Configuration
Enable Service Level Authorization
Hadoop Services and Configuration Properties
Access Control Lists
Refreshing Service Level Authorization Configuration
Examples
Purpose
This document describes how to configure and manage Service Level Authorization for Hadoop.

Prerequisites
Make sure Hadoop is installed, configured and setup correctly. For more information see:

Single Node Setup for first-time users.
Cluster Setup for large, distributed clusters.
Overview
Service Level Authorization is the initial authorization mechanism to ensure clients connecting to a particular Hadoop service have the necessary, pre-configured, permissions and are authorized to access the given service. For example, a MapReduce cluster can use this mechanism to allow a configured list of users/groups to submit jobs.

The ${HADOOP_CONF_DIR}/hadoop-policy.xml configuration file is used to define the access control lists for various Hadoop services.

Service Level Authorization is performed much before to other access control checks such as file-permission checks, access control on job queues etc.

Configuration
This section describes how to configure service-level authorization via the configuration file {HADOOP_CONF_DIR}/hadoop-policy.xml.

Enable Service Level Authorization
By default, service-level authorization is disabled for Hadoop. To enable it set the configuration property hadoop.security.authorization to true in ${HADOOP_CONF_DIR}/core-site.xml.

Hadoop Services and Configuration Properties
This section lists the various Hadoop services and their configuration knobs:

Property	Service
security.client.protocol.acl	ACL for ClientProtocol, which is used by user code via the DistributedFileSystem.
security.client.datanode.protocol.acl	ACL for ClientDatanodeProtocol, the client-to-datanode protocol for block recovery.
security.datanode.protocol.acl	ACL for DatanodeProtocol, which is used by datanodes to communicate with the namenode.
security.inter.datanode.protocol.acl	ACL for InterDatanodeProtocol, the inter-datanode protocol for updating generation timestamp.
security.namenode.protocol.acl	ACL for NamenodeProtocol, the protocol used by the secondary namenode to communicate with the namenode.
security.inter.tracker.protocol.acl	ACL for InterTrackerProtocol, used by the tasktrackers to communicate with the jobtracker.
security.job.submission.protocol.acl	ACL for JobSubmissionProtocol, used by job clients to communciate with the jobtracker for job submission, querying job status etc.
security.task.umbilical.protocol.acl	ACL for TaskUmbilicalProtocol, used by the map and reduce tasks to communicate with the parent tasktracker.
security.refresh.policy.protocol.acl	ACL for RefreshAuthorizationPolicyProtocol, used by the dfsadmin and mradmin commands to refresh the security policy in-effect.
Access Control Lists
${HADOOP_CONF_DIR}/hadoop-policy.xml defines an access control list for each Hadoop service. Every access control list has a simple format:

The list of users and groups are both comma separated list of names. The two lists are separated by a space.

Example: user1,user2 group1,group2.

Add a blank at the beginning of the line if only a list of groups is to be provided, equivalently a comman-separated list of users followed by a space or nothing implies only a set of given users.

A special value of * implies that all users are allowed to access the service.

Refreshing Service Level Authorization Configuration
The service-level authorization configuration for the NameNode and JobTracker can be changed without restarting either of the Hadoop master daemons. The cluster administrator can change ${HADOOP_CONF_DIR}/hadoop-policy.xml on the master nodes and instruct the NameNode and JobTracker to reload their respective configurations via the -refreshServiceAcl switch to dfsadmin and mradmin commands respectively.

Refresh the service-level authorization configuration for the NameNode:

$ bin/hadoop dfsadmin -refreshServiceAcl

Refresh the service-level authorization configuration for the JobTracker:

$ bin/hadoop mradmin -refreshServiceAcl

Of course, one can use the security.refresh.policy.protocol.acl property in ${HADOOP_CONF_DIR}/hadoop-policy.xml to restrict access to the ability to refresh the service-level authorization configuration to certain users/groups.

Examples
Allow only users alice, bob and users in the mapreduce group to submit jobs to the MapReduce cluster:

<property>
     <name>security.job.submission.protocol.acl</name>
     <value>alice,bob mapreduce</value>
</property>
Allow only DataNodes running as the users who belong to the group datanodes to communicate with the NameNode:

<property>
     <name>security.datanode.protocol.acl</name>
     <value>datanodes</value>
</property>
Allow any user to talk to the HDFS cluster as a DFSClient:

<property>
     <name>security.client.protocol.acl</name>
     <value>*</value>
</property>
 
Last Published: 08/04/2013 08:13:21
Copyright © 2008 The Apache Software Foundation.
Apache > Hadoop > Core > docs > r1.2.1
Hadoop Hadoop
   
ProjectWikiHadoop 1.2.1 Documentation
Last Published: 08/04/2013 08:13:21
 
Getting Started
Guides
MapReduce
HDFS
Common
Deployment Layout
File System Shell
Service Level Authorization
Native Libraries
Miscellaneous

PDF -icon
PDF
Service Level Authorization Guide
Purpose
Prerequisites
Overview
Configuration
Enable Service Level Authorization
Hadoop Services and Configuration Properties
Access Control Lists
Refreshing Service Level Authorization Configuration
Examples
Purpose
This document describes how to configure and manage Service Level Authorization for Hadoop.

Prerequisites
Make sure Hadoop is installed, configured and setup correctly. For more information see:

Single Node Setup for first-time users.
Cluster Setup for large, distributed clusters.
Overview
Service Level Authorization is the initial authorization mechanism to ensure clients connecting to a particular Hadoop service have the necessary, pre-configured, permissions and are authorized to access the given service. For example, a MapReduce cluster can use this mechanism to allow a configured list of users/groups to submit jobs.

The ${HADOOP_CONF_DIR}/hadoop-policy.xml configuration file is used to define the access control lists for various Hadoop services.

Service Level Authorization is performed much before to other access control checks such as file-permission checks, access control on job queues etc.

Configuration
This section describes how to configure service-level authorization via the configuration file {HADOOP_CONF_DIR}/hadoop-policy.xml.

Enable Service Level Authorization
By default, service-level authorization is disabled for Hadoop. To enable it set the configuration property hadoop.security.authorization to true in ${HADOOP_CONF_DIR}/core-site.xml.

Hadoop Services and Configuration Properties
This section lists the various Hadoop services and their configuration knobs:

Property	Service
security.client.protocol.acl	ACL for ClientProtocol, which is used by user code via the DistributedFileSystem.
security.client.datanode.protocol.acl	ACL for ClientDatanodeProtocol, the client-to-datanode protocol for block recovery.
security.datanode.protocol.acl	ACL for DatanodeProtocol, which is used by datanodes to communicate with the namenode.
security.inter.datanode.protocol.acl	ACL for InterDatanodeProtocol, the inter-datanode protocol for updating generation timestamp.
security.namenode.protocol.acl	ACL for NamenodeProtocol, the protocol used by the secondary namenode to communicate with the namenode.
security.inter.tracker.protocol.acl	ACL for InterTrackerProtocol, used by the tasktrackers to communicate with the jobtracker.
security.job.submission.protocol.acl	ACL for JobSubmissionProtocol, used by job clients to communciate with the jobtracker for job submission, querying job status etc.
security.task.umbilical.protocol.acl	ACL for TaskUmbilicalProtocol, used by the map and reduce tasks to communicate with the parent tasktracker.
security.refresh.policy.protocol.acl	ACL for RefreshAuthorizationPolicyProtocol, used by the dfsadmin and mradmin commands to refresh the security policy in-effect.
Access Control Lists
${HADOOP_CONF_DIR}/hadoop-policy.xml defines an access control list for each Hadoop service. Every access control list has a simple format:

The list of users and groups are both comma separated list of names. The two lists are separated by a space.

Example: user1,user2 group1,group2.

Add a blank at the beginning of the line if only a list of groups is to be provided, equivalently a comman-separated list of users followed by a space or nothing implies only a set of given users.

A special value of * implies that all users are allowed to access the service.

Refreshing Service Level Authorization Configuration
The service-level authorization configuration for the NameNode and JobTracker can be changed without restarting either of the Hadoop master daemons. The cluster administrator can change ${HADOOP_CONF_DIR}/hadoop-policy.xml on the master nodes and instruct the NameNode and JobTracker to reload their respective configurations via the -refreshServiceAcl switch to dfsadmin and mradmin commands respectively.

Refresh the service-level authorization configuration for the NameNode:

$ bin/hadoop dfsadmin -refreshServiceAcl

Refresh the service-level authorization configuration for the JobTracker:

$ bin/hadoop mradmin -refreshServiceAcl

Of course, one can use the security.refresh.policy.protocol.acl property in ${HADOOP_CONF_DIR}/hadoop-policy.xml to restrict access to the ability to refresh the service-level authorization configuration to certain users/groups.

Examples
Allow only users alice, bob and users in the mapreduce group to submit jobs to the MapReduce cluster:

<property>
     <name>security.job.submission.protocol.acl</name>
     <value>alice,bob mapreduce</value>
</property>
Allow only DataNodes running as the users who belong to the group datanodes to communicate with the NameNode:

<property>
     <name>security.datanode.protocol.acl</name>
     <value>datanodes</value>
</property>
Allow any user to talk to the HDFS cluster as a DFSClient:

<property>
     <name>security.client.protocol.acl</name>
     <value>*</value>
</property>
 
Last Published: 08/04/2013 08:13:21
Copyright © 2008 The Apache Software Foundation.
Apache > Hadoop > Core > docs > r1.2.1
Hadoop Hadoop
   
ProjectWikiHadoop 1.2.1 Documentation
Last Published: 08/04/2013 08:13:21
 
Getting Started
Guides
MapReduce
HDFS
Common
Deployment Layout
File System Shell
Service Level Authorization
Native Libraries
Miscellaneous

PDF -icon
PDF
Service Level Authorization Guide
Purpose
Prerequisites
Overview
Configuration
Enable Service Level Authorization
Hadoop Services and Configuration Properties
Access Control Lists
Refreshing Service Level Authorization Configuration
Examples
Purpose
This document describes how to configure and manage Service Level Authorization for Hadoop.

Prerequisites
Make sure Hadoop is installed, configured and setup correctly. For more information see:

Single Node Setup for first-time users.
Cluster Setup for large, distributed clusters.
Overview
Service Level Authorization is the initial authorization mechanism to ensure clients connecting to a particular Hadoop service have the necessary, pre-configured, permissions and are authorized to access the given service. For example, a MapReduce cluster can use this mechanism to allow a configured list of users/groups to submit jobs.

The ${HADOOP_CONF_DIR}/hadoop-policy.xml configuration file is used to define the access control lists for various Hadoop services.

Service Level Authorization is performed much before to other access control checks such as file-permission checks, access control on job queues etc.

Configuration
This section describes how to configure service-level authorization via the configuration file {HADOOP_CONF_DIR}/hadoop-policy.xml.

Enable Service Level Authorization
By default, service-level authorization is disabled for Hadoop. To enable it set the configuration property hadoop.security.authorization to true in ${HADOOP_CONF_DIR}/core-site.xml.

Hadoop Services and Configuration Properties
This section lists the various Hadoop services and their configuration knobs:

Property	Service
security.client.protocol.acl	ACL for ClientProtocol, which is used by user code via the DistributedFileSystem.
security.client.datanode.protocol.acl	ACL for ClientDatanodeProtocol, the client-to-datanode protocol for block recovery.
security.datanode.protocol.acl	ACL for DatanodeProtocol, which is used by datanodes to communicate with the namenode.
security.inter.datanode.protocol.acl	ACL for InterDatanodeProtocol, the inter-datanode protocol for updating generation timestamp.
security.namenode.protocol.acl	ACL for NamenodeProtocol, the protocol used by the secondary namenode to communicate with the namenode.
security.inter.tracker.protocol.acl	ACL for InterTrackerProtocol, used by the tasktrackers to communicate with the jobtracker.
security.job.submission.protocol.acl	ACL for JobSubmissionProtocol, used by job clients to communciate with the jobtracker for job submission, querying job status etc.
security.task.umbilical.protocol.acl	ACL for TaskUmbilicalProtocol, used by the map and reduce tasks to communicate with the parent tasktracker.
security.refresh.policy.protocol.acl	ACL for RefreshAuthorizationPolicyProtocol, used by the dfsadmin and mradmin commands to refresh the security policy in-effect.
Access Control Lists
${HADOOP_CONF_DIR}/hadoop-policy.xml defines an access control list for each Hadoop service. Every access control list has a simple format:

The list of users and groups are both comma separated list of names. The two lists are separated by a space.

Example: user1,user2 group1,group2.

Add a blank at the beginning of the line if only a list of groups is to be provided, equivalently a comman-separated list of users followed by a space or nothing implies only a set of given users.

A special value of * implies that all users are allowed to access the service.

Refreshing Service Level Authorization Configuration
The service-level authorization configuration for the NameNode and JobTracker can be changed without restarting either of the Hadoop master daemons. The cluster administrator can change ${HADOOP_CONF_DIR}/hadoop-policy.xml on the master nodes and instruct the NameNode and JobTracker to reload their respective configurations via the -refreshServiceAcl switch to dfsadmin and mradmin commands respectively.

Refresh the service-level authorization configuration for the NameNode:

$ bin/hadoop dfsadmin -refreshServiceAcl

Refresh the service-level authorization configuration for the JobTracker:

$ bin/hadoop mradmin -refreshServiceAcl

Of course, one can use the security.refresh.policy.protocol.acl property in ${HADOOP_CONF_DIR}/hadoop-policy.xml to restrict access to the ability to refresh the service-level authorization configuration to certain users/groups.

Examples
Allow only users alice, bob and users in the mapreduce group to submit jobs to the MapReduce cluster:

<property>
     <name>security.job.submission.protocol.acl</name>
     <value>alice,bob mapreduce</value>
</property>
Allow only DataNodes running as the users who belong to the group datanodes to communicate with the NameNode:

<property>
     <name>security.datanode.protocol.acl</name>
     <value>datanodes</value>
</property>
Allow any user to talk to the HDFS cluster as a DFSClient:

<property>
     <name>security.client.protocol.acl</name>
     <value>*</value>
</property>
 
Last Published: 08/04/2013 08:13:21
Copyright © 2008 The Apache Software Foundation.
Apache > Hadoop > Core > docs > r1.2.1
Hadoop Hadoop
   
ProjectWikiHadoop 1.2.1 Documentation
Last Published: 08/04/2013 08:13:21
 
Getting Started
Guides
MapReduce
HDFS
Common
Deployment Layout
File System Shell
Service Level Authorization
Native Libraries
Miscellaneous

PDF -icon
PDF
Service Level Authorization Guide
Purpose
Prerequisites
Overview
Configuration
Enable Service Level Authorization
Hadoop Services and Configuration Properties
Access Control Lists
Refreshing Service Level Authorization Configuration
Examples
Purpose
This document describes how to configure and manage Service Level Authorization for Hadoop.

Prerequisites
Make sure Hadoop is installed, configured and setup correctly. For more information see:

Single Node Setup for first-time users.
Cluster Setup for large, distributed clusters.
Overview
Service Level Authorization is the initial authorization mechanism to ensure clients connecting to a particular Hadoop service have the necessary, pre-configured, permissions and are authorized to access the given service. For example, a MapReduce cluster can use this mechanism to allow a configured list of users/groups to submit jobs.

The ${HADOOP_CONF_DIR}/hadoop-policy.xml configuration file is used to define the access control lists for various Hadoop services.

Service Level Authorization is performed much before to other access control checks such as file-permission checks, access control on job queues etc.

Configuration
This section describes how to configure service-level authorization via the configuration file {HADOOP_CONF_DIR}/hadoop-policy.xml.

Enable Service Level Authorization
By default, service-level authorization is disabled for Hadoop. To enable it set the configuration property hadoop.security.authorization to true in ${HADOOP_CONF_DIR}/core-site.xml.

Hadoop Services and Configuration Properties
This section lists the various Hadoop services and their configuration knobs:

Property	Service
security.client.protocol.acl	ACL for ClientProtocol, which is used by user code via the DistributedFileSystem.
security.client.datanode.protocol.acl	ACL for ClientDatanodeProtocol, the client-to-datanode protocol for block recovery.
security.datanode.protocol.acl	ACL for DatanodeProtocol, which is used by datanodes to communicate with the namenode.
security.inter.datanode.protocol.acl	ACL for InterDatanodeProtocol, the inter-datanode protocol for updating generation timestamp.
security.namenode.protocol.acl	ACL for NamenodeProtocol, the protocol used by the secondary namenode to communicate with the namenode.
security.inter.tracker.protocol.acl	ACL for InterTrackerProtocol, used by the tasktrackers to communicate with the jobtracker.
security.job.submission.protocol.acl	ACL for JobSubmissionProtocol, used by job clients to communciate with the jobtracker for job submission, querying job status etc.
security.task.umbilical.protocol.acl	ACL for TaskUmbilicalProtocol, used by the map and reduce tasks to communicate with the parent tasktracker.
security.refresh.policy.protocol.acl	ACL for RefreshAuthorizationPolicyProtocol, used by the dfsadmin and mradmin commands to refresh the security policy in-effect.
Access Control Lists
${HADOOP_CONF_DIR}/hadoop-policy.xml defines an access control list for each Hadoop service. Every access control list has a simple format:

The list of users and groups are both comma separated list of names. The two lists are separated by a space.

Example: user1,user2 group1,group2.

Add a blank at the beginning of the line if only a list of groups is to be provided, equivalently a comman-separated list of users followed by a space or nothing implies only a set of given users.

A special value of * implies that all users are allowed to access the service.

Refreshing Service Level Authorization Configuration
The service-level authorization configuration for the NameNode and JobTracker can be changed without restarting either of the Hadoop master daemons. The cluster administrator can change ${HADOOP_CONF_DIR}/hadoop-policy.xml on the master nodes and instruct the NameNode and JobTracker to reload their respective configurations via the -refreshServiceAcl switch to dfsadmin and mradmin commands respectively.

Refresh the service-level authorization configuration for the NameNode:

$ bin/hadoop dfsadmin -refreshServiceAcl

Refresh the service-level authorization configuration for the JobTracker:

$ bin/hadoop mradmin -refreshServiceAcl

Of course, one can use the security.refresh.policy.protocol.acl property in ${HADOOP_CONF_DIR}/hadoop-policy.xml to restrict access to the ability to refresh the service-level authorization configuration to certain users/groups.

Examples
Allow only users alice, bob and users in the mapreduce group to submit jobs to the MapReduce cluster:

<property>
     <name>security.job.submission.protocol.acl</name>
     <value>alice,bob mapreduce</value>
</property>
Allow only DataNodes running as the users who belong to the group datanodes to communicate with the NameNode:

<property>
     <name>security.datanode.protocol.acl</name>
     <value>datanodes</value>
</property>
Allow any user to talk to the HDFS cluster as a DFSClient:

<property>
     <name>security.client.protocol.acl</name>
     <value>*</value>
</property>
 
Last Published: 08/04/2013 08:13:21
Copyright © 2008 The Apache Software Foundation.
Apache > Hadoop > Core > docs > r1.2.1
Hadoop Hadoop
   
ProjectWikiHadoop 1.2.1 Documentation
Last Published: 08/04/2013 08:13:21
 
Getting Started
Guides
MapReduce
HDFS
Common
Deployment Layout
File System Shell
Service Level Authorization
Native Libraries
Miscellaneous

PDF -icon
PDF
Service Level Authorization Guide
Purpose
Prerequisites
Overview
Configuration
Enable Service Level Authorization
Hadoop Services and Configuration Properties
Access Control Lists
Refreshing Service Level Authorization Configuration
Examples
Purpose
This document describes how to configure and manage Service Level Authorization for Hadoop.

Prerequisites
Make sure Hadoop is installed, configured and setup correctly. For more information see:

Single Node Setup for first-time users.
Cluster Setup for large, distributed clusters.
Overview
Service Level Authorization is the initial authorization mechanism to ensure clients connecting to a particular Hadoop service have the necessary, pre-configured, permissions and are authorized to access the given service. For example, a MapReduce cluster can use this mechanism to allow a configured list of users/groups to submit jobs.

The ${HADOOP_CONF_DIR}/hadoop-policy.xml configuration file is used to define the access control lists for various Hadoop services.

Service Level Authorization is performed much before to other access control checks such as file-permission checks, access control on job queues etc.

Configuration
This section describes how to configure service-level authorization via the configuration file {HADOOP_CONF_DIR}/hadoop-policy.xml.

Enable Service Level Authorization
By default, service-level authorization is disabled for Hadoop. To enable it set the configuration property hadoop.security.authorization to true in ${HADOOP_CONF_DIR}/core-site.xml.

Hadoop Services and Configuration Properties
This section lists the various Hadoop services and their configuration knobs:

Property	Service
security.client.protocol.acl	ACL for ClientProtocol, which is used by user code via the DistributedFileSystem.
security.client.datanode.protocol.acl	ACL for ClientDatanodeProtocol, the client-to-datanode protocol for block recovery.
security.datanode.protocol.acl	ACL for DatanodeProtocol, which is used by datanodes to communicate with the namenode.
security.inter.datanode.protocol.acl	ACL for InterDatanodeProtocol, the inter-datanode protocol for updating generation timestamp.
security.namenode.protocol.acl	ACL for NamenodeProtocol, the protocol used by the secondary namenode to communicate with the namenode.
security.inter.tracker.protocol.acl	ACL for InterTrackerProtocol, used by the tasktrackers to communicate with the jobtracker.
security.job.submission.protocol.acl	ACL for JobSubmissionProtocol, used by job clients to communciate with the jobtracker for job submission, querying job status etc.
security.task.umbilical.protocol.acl	ACL for TaskUmbilicalProtocol, used by the map and reduce tasks to communicate with the parent tasktracker.
security.refresh.policy.protocol.acl	ACL for RefreshAuthorizationPolicyProtocol, used by the dfsadmin and mradmin commands to refresh the security policy in-effect.
Access Control Lists
${HADOOP_CONF_DIR}/hadoop-policy.xml defines an access control list for each Hadoop service. Every access control list has a simple format:

The list of users and groups are both comma separated list of names. The two lists are separated by a space.

Example: user1,user2 group1,group2.

Add a blank at the beginning of the line if only a list of groups is to be provided, equivalently a comman-separated list of users followed by a space or nothing implies only a set of given users.

A special value of * implies that all users are allowed to access the service.

Refreshing Service Level Authorization Configuration
The service-level authorization configuration for the NameNode and JobTracker can be changed without restarting either of the Hadoop master daemons. The cluster administrator can change ${HADOOP_CONF_DIR}/hadoop-policy.xml on the master nodes and instruct the NameNode and JobTracker to reload their respective configurations via the -refreshServiceAcl switch to dfsadmin and mradmin commands respectively.

Refresh the service-level authorization configuration for the NameNode:

$ bin/hadoop dfsadmin -refreshServiceAcl

Refresh the service-level authorization configuration for the JobTracker:

$ bin/hadoop mradmin -refreshServiceAcl

Of course, one can use the security.refresh.policy.protocol.acl property in ${HADOOP_CONF_DIR}/hadoop-policy.xml to restrict access to the ability to refresh the service-level authorization configuration to certain users/groups.

Examples
Allow only users alice, bob and users in the mapreduce group to submit jobs to the MapReduce cluster:

<property>
     <name>security.job.submission.protocol.acl</name>
     <value>alice,bob mapreduce</value>
</property>
Allow only DataNodes running as the users who belong to the group datanodes to communicate with the NameNode:

<property>
     <name>security.datanode.protocol.acl</name>
     <value>datanodes</value>
</property>
Allow any user to talk to the HDFS cluster as a DFSClient:

<property>
     <name>security.client.protocol.acl</name>
     <value>*</value>
</property>
 
Last Published: 08/04/2013 08:13:21
Copyright © 2008 The Apache Software Foundation.
The Apache Software Foundation
The Apache Software Foundation

Community-led development since 1999.
FoundationProjectsPeopleGet InvolvedDownloadSupport ApacheHome  
We consider ourselves
not simply a group of projects sharing a server, but rather a community of developers and users.
The Apache Software Foundation
provides support for the Apache community of open-source software projects, which provide software products for the public good.
The Apache projects are defined
by collaborative consensus based processes, an open, pragmatic software license and a desire to create high quality software that leads the way in its field.
Featured Projects »Apache TomEE Apache OpenMeetings Apache Tobago
The ASF is made up of nearly 150
top level projects which cover a wide range of technologies. Chances are if you are looking for a rewarding experience in Open Source, you are going to find it here.
Apache TomEE
Apache TomEE is an all-Apache Java EE 6 Web Profile stack for Apache Tomcat
Apache TomEE Web Profile delivers Servlets, JSP, JSF, JTA, JPA, CDI, Bean Validation and EJB Lite. Apache TomEE Plus has all the features of TomEE with the addition of JAX-RS (RESTfull Services), JAX-WS (Web Services), JMS (Java Message Service) and JCA (the Java Connector Architecture). The additional functionality is delivered via Apache CXF, Apache ActiveMQ and the Geronimo Connector library
Latest News
If you would like to keep up with news and announcements from the foundation and all its projects, you can subscribe to the Apache Announcements List or follow the Foundation Blog.

The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open Source projects and initiatives, announced today that...
The Apache Software Foundation Announces Apache™ Log4j™ v2
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture.

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open Source projects and initiatives, announced today the General Availability of Apache™...
Travel Assistance Applications Now Open for ApacheCon Europe: Budapest 17-21 November 2014
The Apache Software Foundation (ASF)'s Travel Assistance Committee (TAC) is now accepting applications for ApacheCon Europe 2014, taking place 17-21 November in Budapest, Hungary.

The TAC is seeking individuals from the Apache community at-large, users, developers, educators, students, Committers, and Members, who would like to attend ApacheCon, but need some...
The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
> This announcement is also available online at http://s.apache.org/keG
 
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 
 
Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of...
The Apache Software Foundation Announces Apache™ Log4j™ v2
> This announcement is also available online at http://s.apache.org/wm
 
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture. 
 
Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open...
[ANNOUNCE] Apache Jackrabbit Oak 1.0.2 released
The Apache Jackrabbit community is pleased to announce the release of Apache Jackrabbit Oak 1.0.2. The release is available for download at:
 
http://jackrabbit.apache.org/downloads.html
 
See the full release notes below for details about this release.
 
Release Notes -- Apache Jackrabbit Oak -- Version 1.0.2
 
Introduction ------------
 ...
Latest Activity
This is an overview of activity going on with our projects. SVN commits, bug reports, tweets, you name it.

@TheASF: The Apache Software Foundation Announces #Apache™ #Tez™ as a Top-Level Project http://t.co/P6cJ5nbcMQ #OpenSource #BigData #Hadoop #YARN
@TheASF: The Apache Software Foundation Announces #Apache™ #Log4j™ v2 http://t.co/zhk0cTIzAf #OpenSource #logging #library #Java #plugin
@TheASF: Join #Apache @OSCON: Booth 13 + presos on #HTTP Server #Cassandra #Mesos #Cordova #CloudStack #CouchDB #Hadoop #Spark #TrafficServer & more!
r1612633 importing geronimo jaxrs 2 and versioning it 7 (tomee) — rmannibucau
r1612632 HDFS-6720. Remove KeyProvider in EncryptionZoneManager. (wang) (hadoop) — wang
r1612631 HADOOP-10818. native client: refactor URI code to be clearer (cmccabe) (hadoop) — cmccabe
r1612630 fixing NOTICE (geronimo) — rmannibucau
r1612629 Merge r1612626 from trunk. YARN-2295. Refactored DistributedShell to use public APIs of protocol records. Contributed by Li Lu (hadoop) — jianhe
[PDFBOX-2226] IndexOutOfBoundsException when merging many PDFs in memory
An IndexOutOfBoundsException occurs when attempting to merge many (at least 10) PDF documents together. All PDFs exist in byte arrays...

[AURORA-595] test_cluster_option test case failing 
Test src.test.python.apache.aurora.common.test_cluster_option ..... FAILURE

E AttributeError: CaptureFixture instance has no attribute '_outerr'

[LUCENE-5825] Allowing the benchmarking algorithm to choose PostingsFormat
The algorithm file for benchmarking should allow PostingsFormat to be configurable.

[SENTRY-354] Test for update.distrib phase overriding
NOTE: I left the Fix Version blank because I think this should go into 1.5.0, i.e. after 1.4.0 is branched,...

[MESOS-1193] Check failed: promises.contains(containerId) crashes slave
This was observed with four slaves on one machine, one framework (Marathon) and around 100 tasks per slave.

I0404 17:58:58.298075...

Projects
HTTP Server
Abdera
Accumulo
ACE
ActiveMQ
Airavata
Allura
Ambari
Ant
Any23
APR
Archiva
Aries
Avro
Axis
Bigtop
Bloodhound
Buildr
BVal
Camel
Cassandra
Cayenne
Chemistry
Chukwa
Clerezza
CloudStack
Cocoon
Commons
Continuum
Cordova
CouchDB
Creadur
Crunch
cTAKES
Curator
CXF
DB
Deltacloud
DeltaSpike
DirectMemory
Directory
Empire-db
Etch
Felix
Flex
Flume
Forrest
Geronimo
Giraph
Gora
Gump
Hadoop
Hama
HBase
Helix
Hive
HttpComponents
Isis
Jackrabbit
James
jclouds
Jena
JMeter
JSPWiki
jUDDI
Kafka
Karaf
Knox
Lenya
Libcloud
Logging
Lucene
Lucene.Net
Lucy
Mahout
ManifoldCF
Marmotta
Maven
Mesos
MINA
MRUnit
MyFaces
Nutch
ODE
OFBiz
Olingo
Oltu
Onami
OODT
Oozie
Open Climate Workbench
OpenJPA
OpenMeetings
OpenNLP
OpenOffice
OpenWebBeans
PDFBox
Perl
Pig
Pivot
POI
Portals
Qpid
Rave
River
Roller
Santuario
ServiceMix
Shindig
Shiro
SIS
Sling
SpamAssassin
Spark
Sqoop
Stanbol
STeVe
Struts
Subversion
Synapse
Syncope
Tajo
Tapestry
Tcl
Thrift
Tika
Tiles
Tomcat
TomEE
Traffic Server
Turbine
Tuscany
UIMA
VCL
Velocity
Web Services
Whirr
Wicket
Wink
Wookie
Xalan
Xerces
XMLBeans
XML Graphics
ZooKeeper
Foundation
FAQ
Glossary
Licenses
Trademarks
News
Press Inquiries
Public Records
Mailing Lists
Sponsorship
Donations
Buy Stuff
Thanks
Contact
Foundation Projects
Attic
Conferences
Community Development
Incubator
Infrastructure
Labs
Legal Affairs
Public Relations
Security
Travel Assistance
The Apache Blogs

The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 

Forest Hill, MD –22 July 2014– The Apache Software...

The Apache Software Foundation Announces Apache™ Log4j™ v2
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture.

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer...

Apache Sentry architecture overview
Apache Sentry architecture overview

Apache Sentry is an authorization module for Hadoop that provides the granular, role-based authorization required to provide precise levels of access to the right users and...


Read More »
Copyright © 2012 The Apache Software Foundation, Licensed under the Apache License, Version 2.0.
Apache and the Apache feather logo are trademarks of The Apache Software Foundation.
The Apache Software Foundation
The Apache Software Foundation

Community-led development since 1999.
FoundationProjectsPeopleGet InvolvedDownloadSupport ApacheHome  
We consider ourselves
not simply a group of projects sharing a server, but rather a community of developers and users.
The Apache Software Foundation
provides support for the Apache community of open-source software projects, which provide software products for the public good.
The Apache projects are defined
by collaborative consensus based processes, an open, pragmatic software license and a desire to create high quality software that leads the way in its field.
Featured Projects »Apache TomEE Apache OpenMeetings Apache Tobago
The ASF is made up of nearly 150
top level projects which cover a wide range of technologies. Chances are if you are looking for a rewarding experience in Open Source, you are going to find it here.
Apache TomEE
Apache TomEE is an all-Apache Java EE 6 Web Profile stack for Apache Tomcat
Apache TomEE Web Profile delivers Servlets, JSP, JSF, JTA, JPA, CDI, Bean Validation and EJB Lite. Apache TomEE Plus has all the features of TomEE with the addition of JAX-RS (RESTfull Services), JAX-WS (Web Services), JMS (Java Message Service) and JCA (the Java Connector Architecture). The additional functionality is delivered via Apache CXF, Apache ActiveMQ and the Geronimo Connector library
Latest News
If you would like to keep up with news and announcements from the foundation and all its projects, you can subscribe to the Apache Announcements List or follow the Foundation Blog.

The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open Source projects and initiatives, announced today that...
The Apache Software Foundation Announces Apache™ Log4j™ v2
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture.

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open Source projects and initiatives, announced today the General Availability of Apache™...
Travel Assistance Applications Now Open for ApacheCon Europe: Budapest 17-21 November 2014
The Apache Software Foundation (ASF)'s Travel Assistance Committee (TAC) is now accepting applications for ApacheCon Europe 2014, taking place 17-21 November in Budapest, Hungary.

The TAC is seeking individuals from the Apache community at-large, users, developers, educators, students, Committers, and Members, who would like to attend ApacheCon, but need some...
The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
> This announcement is also available online at http://s.apache.org/keG
 
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 
 
Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of...
The Apache Software Foundation Announces Apache™ Log4j™ v2
> This announcement is also available online at http://s.apache.org/wm
 
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture. 
 
Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open...
[ANNOUNCE] Apache Jackrabbit Oak 1.0.2 released
The Apache Jackrabbit community is pleased to announce the release of Apache Jackrabbit Oak 1.0.2. The release is available for download at:
 
http://jackrabbit.apache.org/downloads.html
 
See the full release notes below for details about this release.
 
Release Notes -- Apache Jackrabbit Oak -- Version 1.0.2
 
Introduction ------------
 ...
Latest Activity
This is an overview of activity going on with our projects. SVN commits, bug reports, tweets, you name it.

@TheASF: The Apache Software Foundation Announces #Apache™ #Tez™ as a Top-Level Project http://t.co/P6cJ5nbcMQ #OpenSource #BigData #Hadoop #YARN
@TheASF: The Apache Software Foundation Announces #Apache™ #Log4j™ v2 http://t.co/zhk0cTIzAf #OpenSource #logging #library #Java #plugin
@TheASF: Join #Apache @OSCON: Booth 13 + presos on #HTTP Server #Cassandra #Mesos #Cordova #CloudStack #CouchDB #Hadoop #Spark #TrafficServer & more!
r1612633 importing geronimo jaxrs 2 and versioning it 7 (tomee) — rmannibucau
r1612632 HDFS-6720. Remove KeyProvider in EncryptionZoneManager. (wang) (hadoop) — wang
r1612631 HADOOP-10818. native client: refactor URI code to be clearer (cmccabe) (hadoop) — cmccabe
r1612630 fixing NOTICE (geronimo) — rmannibucau
r1612629 Merge r1612626 from trunk. YARN-2295. Refactored DistributedShell to use public APIs of protocol records. Contributed by Li Lu (hadoop) — jianhe
[PDFBOX-2226] IndexOutOfBoundsException when merging many PDFs in memory
An IndexOutOfBoundsException occurs when attempting to merge many (at least 10) PDF documents together. All PDFs exist in byte arrays...

[AURORA-595] test_cluster_option test case failing 
Test src.test.python.apache.aurora.common.test_cluster_option ..... FAILURE

E AttributeError: CaptureFixture instance has no attribute '_outerr'

[LUCENE-5825] Allowing the benchmarking algorithm to choose PostingsFormat
The algorithm file for benchmarking should allow PostingsFormat to be configurable.

[SENTRY-354] Test for update.distrib phase overriding
NOTE: I left the Fix Version blank because I think this should go into 1.5.0, i.e. after 1.4.0 is branched,...

[MESOS-1193] Check failed: promises.contains(containerId) crashes slave
This was observed with four slaves on one machine, one framework (Marathon) and around 100 tasks per slave.

I0404 17:58:58.298075...

Projects
HTTP Server
Abdera
Accumulo
ACE
ActiveMQ
Airavata
Allura
Ambari
Ant
Any23
APR
Archiva
Aries
Avro
Axis
Bigtop
Bloodhound
Buildr
BVal
Camel
Cassandra
Cayenne
Chemistry
Chukwa
Clerezza
CloudStack
Cocoon
Commons
Continuum
Cordova
CouchDB
Creadur
Crunch
cTAKES
Curator
CXF
DB
Deltacloud
DeltaSpike
DirectMemory
Directory
Empire-db
Etch
Felix
Flex
Flume
Forrest
Geronimo
Giraph
Gora
Gump
Hadoop
Hama
HBase
Helix
Hive
HttpComponents
Isis
Jackrabbit
James
jclouds
Jena
JMeter
JSPWiki
jUDDI
Kafka
Karaf
Knox
Lenya
Libcloud
Logging
Lucene
Lucene.Net
Lucy
Mahout
ManifoldCF
Marmotta
Maven
Mesos
MINA
MRUnit
MyFaces
Nutch
ODE
OFBiz
Olingo
Oltu
Onami
OODT
Oozie
Open Climate Workbench
OpenJPA
OpenMeetings
OpenNLP
OpenOffice
OpenWebBeans
PDFBox
Perl
Pig
Pivot
POI
Portals
Qpid
Rave
River
Roller
Santuario
ServiceMix
Shindig
Shiro
SIS
Sling
SpamAssassin
Spark
Sqoop
Stanbol
STeVe
Struts
Subversion
Synapse
Syncope
Tajo
Tapestry
Tcl
Thrift
Tika
Tiles
Tomcat
TomEE
Traffic Server
Turbine
Tuscany
UIMA
VCL
Velocity
Web Services
Whirr
Wicket
Wink
Wookie
Xalan
Xerces
XMLBeans
XML Graphics
ZooKeeper
Foundation
FAQ
Glossary
Licenses
Trademarks
News
Press Inquiries
Public Records
Mailing Lists
Sponsorship
Donations
Buy Stuff
Thanks
Contact
Foundation Projects
Attic
Conferences
Community Development
Incubator
Infrastructure
Labs
Legal Affairs
Public Relations
Security
Travel Assistance
The Apache Blogs

The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 

Forest Hill, MD –22 July 2014– The Apache Software...

The Apache Software Foundation Announces Apache™ Log4j™ v2
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture.

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer...

Apache Sentry architecture overview
Apache Sentry architecture overview

Apache Sentry is an authorization module for Hadoop that provides the granular, role-based authorization required to provide precise levels of access to the right users and...


Read More »
Copyright © 2012 The Apache Software Foundation, Licensed under the Apache License, Version 2.0.
Apache and the Apache feather logo are trademarks of The Apache Software Foundation.
The Apache Software Foundation
The Apache Software Foundation

Community-led development since 1999.
FoundationProjectsPeopleGet InvolvedDownloadSupport ApacheHome  
We consider ourselves
not simply a group of projects sharing a server, but rather a community of developers and users.
The Apache Software Foundation
provides support for the Apache community of open-source software projects, which provide software products for the public good.
The Apache projects are defined
by collaborative consensus based processes, an open, pragmatic software license and a desire to create high quality software that leads the way in its field.
Featured Projects »Apache TomEE Apache OpenMeetings Apache Tobago
The ASF is made up of nearly 150
top level projects which cover a wide range of technologies. Chances are if you are looking for a rewarding experience in Open Source, you are going to find it here.
Apache TomEE
Apache TomEE is an all-Apache Java EE 6 Web Profile stack for Apache Tomcat
Apache TomEE Web Profile delivers Servlets, JSP, JSF, JTA, JPA, CDI, Bean Validation and EJB Lite. Apache TomEE Plus has all the features of TomEE with the addition of JAX-RS (RESTfull Services), JAX-WS (Web Services), JMS (Java Message Service) and JCA (the Java Connector Architecture). The additional functionality is delivered via Apache CXF, Apache ActiveMQ and the Geronimo Connector library
Latest News
If you would like to keep up with news and announcements from the foundation and all its projects, you can subscribe to the Apache Announcements List or follow the Foundation Blog.

The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open Source projects and initiatives, announced today that...
The Apache Software Foundation Announces Apache™ Log4j™ v2
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture.

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open Source projects and initiatives, announced today the General Availability of Apache™...
Travel Assistance Applications Now Open for ApacheCon Europe: Budapest 17-21 November 2014
The Apache Software Foundation (ASF)'s Travel Assistance Committee (TAC) is now accepting applications for ApacheCon Europe 2014, taking place 17-21 November in Budapest, Hungary.

The TAC is seeking individuals from the Apache community at-large, users, developers, educators, students, Committers, and Members, who would like to attend ApacheCon, but need some...
The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
> This announcement is also available online at http://s.apache.org/keG
 
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 
 
Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of...
The Apache Software Foundation Announces Apache™ Log4j™ v2
> This announcement is also available online at http://s.apache.org/wm
 
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture. 
 
Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open...
[ANNOUNCE] Apache Jackrabbit Oak 1.0.2 released
The Apache Jackrabbit community is pleased to announce the release of Apache Jackrabbit Oak 1.0.2. The release is available for download at:
 
http://jackrabbit.apache.org/downloads.html
 
See the full release notes below for details about this release.
 
Release Notes -- Apache Jackrabbit Oak -- Version 1.0.2
 
Introduction ------------
 ...
Latest Activity
This is an overview of activity going on with our projects. SVN commits, bug reports, tweets, you name it.

@TheASF: The Apache Software Foundation Announces #Apache™ #Tez™ as a Top-Level Project http://t.co/P6cJ5nbcMQ #OpenSource #BigData #Hadoop #YARN
@TheASF: The Apache Software Foundation Announces #Apache™ #Log4j™ v2 http://t.co/zhk0cTIzAf #OpenSource #logging #library #Java #plugin
@TheASF: Join #Apache @OSCON: Booth 13 + presos on #HTTP Server #Cassandra #Mesos #Cordova #CloudStack #CouchDB #Hadoop #Spark #TrafficServer & more!
r1612633 importing geronimo jaxrs 2 and versioning it 7 (tomee) — rmannibucau
r1612632 HDFS-6720. Remove KeyProvider in EncryptionZoneManager. (wang) (hadoop) — wang
r1612631 HADOOP-10818. native client: refactor URI code to be clearer (cmccabe) (hadoop) — cmccabe
r1612630 fixing NOTICE (geronimo) — rmannibucau
r1612629 Merge r1612626 from trunk. YARN-2295. Refactored DistributedShell to use public APIs of protocol records. Contributed by Li Lu (hadoop) — jianhe
[PDFBOX-2226] IndexOutOfBoundsException when merging many PDFs in memory
An IndexOutOfBoundsException occurs when attempting to merge many (at least 10) PDF documents together. All PDFs exist in byte arrays...

[AURORA-595] test_cluster_option test case failing 
Test src.test.python.apache.aurora.common.test_cluster_option ..... FAILURE

E AttributeError: CaptureFixture instance has no attribute '_outerr'

[LUCENE-5825] Allowing the benchmarking algorithm to choose PostingsFormat
The algorithm file for benchmarking should allow PostingsFormat to be configurable.

[SENTRY-354] Test for update.distrib phase overriding
NOTE: I left the Fix Version blank because I think this should go into 1.5.0, i.e. after 1.4.0 is branched,...

[MESOS-1193] Check failed: promises.contains(containerId) crashes slave
This was observed with four slaves on one machine, one framework (Marathon) and around 100 tasks per slave.

I0404 17:58:58.298075...

Projects
HTTP Server
Abdera
Accumulo
ACE
ActiveMQ
Airavata
Allura
Ambari
Ant
Any23
APR
Archiva
Aries
Avro
Axis
Bigtop
Bloodhound
Buildr
BVal
Camel
Cassandra
Cayenne
Chemistry
Chukwa
Clerezza
CloudStack
Cocoon
Commons
Continuum
Cordova
CouchDB
Creadur
Crunch
cTAKES
Curator
CXF
DB
Deltacloud
DeltaSpike
DirectMemory
Directory
Empire-db
Etch
Felix
Flex
Flume
Forrest
Geronimo
Giraph
Gora
Gump
Hadoop
Hama
HBase
Helix
Hive
HttpComponents
Isis
Jackrabbit
James
jclouds
Jena
JMeter
JSPWiki
jUDDI
Kafka
Karaf
Knox
Lenya
Libcloud
Logging
Lucene
Lucene.Net
Lucy
Mahout
ManifoldCF
Marmotta
Maven
Mesos
MINA
MRUnit
MyFaces
Nutch
ODE
OFBiz
Olingo
Oltu
Onami
OODT
Oozie
Open Climate Workbench
OpenJPA
OpenMeetings
OpenNLP
OpenOffice
OpenWebBeans
PDFBox
Perl
Pig
Pivot
POI
Portals
Qpid
Rave
River
Roller
Santuario
ServiceMix
Shindig
Shiro
SIS
Sling
SpamAssassin
Spark
Sqoop
Stanbol
STeVe
Struts
Subversion
Synapse
Syncope
Tajo
Tapestry
Tcl
Thrift
Tika
Tiles
Tomcat
TomEE
Traffic Server
Turbine
Tuscany
UIMA
VCL
Velocity
Web Services
Whirr
Wicket
Wink
Wookie
Xalan
Xerces
XMLBeans
XML Graphics
ZooKeeper
Foundation
FAQ
Glossary
Licenses
Trademarks
News
Press Inquiries
Public Records
Mailing Lists
Sponsorship
Donations
Buy Stuff
Thanks
Contact
Foundation Projects
Attic
Conferences
Community Development
Incubator
Infrastructure
Labs
Legal Affairs
Public Relations
Security
Travel Assistance
The Apache Blogs

The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 

Forest Hill, MD –22 July 2014– The Apache Software...

The Apache Software Foundation Announces Apache™ Log4j™ v2
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture.

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer...

Apache Sentry architecture overview
Apache Sentry architecture overview

Apache Sentry is an authorization module for Hadoop that provides the granular, role-based authorization required to provide precise levels of access to the right users and...


Read More »
Copyright © 2012 The Apache Software Foundation, Licensed under the Apache License, Version 2.0.
Apache and the Apache feather logo are trademarks of The Apache Software Foundation.
The Apache Software Foundation
The Apache Software Foundation

Community-led development since 1999.
FoundationProjectsPeopleGet InvolvedDownloadSupport ApacheHome  
We consider ourselves
not simply a group of projects sharing a server, but rather a community of developers and users.
The Apache Software Foundation
provides support for the Apache community of open-source software projects, which provide software products for the public good.
The Apache projects are defined
by collaborative consensus based processes, an open, pragmatic software license and a desire to create high quality software that leads the way in its field.
Featured Projects »Apache TomEE Apache OpenMeetings Apache Tobago
The ASF is made up of nearly 150
top level projects which cover a wide range of technologies. Chances are if you are looking for a rewarding experience in Open Source, you are going to find it here.
Apache TomEE
Apache TomEE is an all-Apache Java EE 6 Web Profile stack for Apache Tomcat
Apache TomEE Web Profile delivers Servlets, JSP, JSF, JTA, JPA, CDI, Bean Validation and EJB Lite. Apache TomEE Plus has all the features of TomEE with the addition of JAX-RS (RESTfull Services), JAX-WS (Web Services), JMS (Java Message Service) and JCA (the Java Connector Architecture). The additional functionality is delivered via Apache CXF, Apache ActiveMQ and the Geronimo Connector library
Latest News
If you would like to keep up with news and announcements from the foundation and all its projects, you can subscribe to the Apache Announcements List or follow the Foundation Blog.

The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open Source projects and initiatives, announced today that...
The Apache Software Foundation Announces Apache™ Log4j™ v2
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture.

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open Source projects and initiatives, announced today the General Availability of Apache™...
Travel Assistance Applications Now Open for ApacheCon Europe: Budapest 17-21 November 2014
The Apache Software Foundation (ASF)'s Travel Assistance Committee (TAC) is now accepting applications for ApacheCon Europe 2014, taking place 17-21 November in Budapest, Hungary.

The TAC is seeking individuals from the Apache community at-large, users, developers, educators, students, Committers, and Members, who would like to attend ApacheCon, but need some...
The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
> This announcement is also available online at http://s.apache.org/keG
 
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 
 
Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of...
The Apache Software Foundation Announces Apache™ Log4j™ v2
> This announcement is also available online at http://s.apache.org/wm
 
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture. 
 
Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open...
[ANNOUNCE] Apache Jackrabbit Oak 1.0.2 released
The Apache Jackrabbit community is pleased to announce the release of Apache Jackrabbit Oak 1.0.2. The release is available for download at:
 
http://jackrabbit.apache.org/downloads.html
 
See the full release notes below for details about this release.
 
Release Notes -- Apache Jackrabbit Oak -- Version 1.0.2
 
Introduction ------------
 ...
Latest Activity
This is an overview of activity going on with our projects. SVN commits, bug reports, tweets, you name it.

@TheASF: The Apache Software Foundation Announces #Apache™ #Tez™ as a Top-Level Project http://t.co/P6cJ5nbcMQ #OpenSource #BigData #Hadoop #YARN
@TheASF: The Apache Software Foundation Announces #Apache™ #Log4j™ v2 http://t.co/zhk0cTIzAf #OpenSource #logging #library #Java #plugin
@TheASF: Join #Apache @OSCON: Booth 13 + presos on #HTTP Server #Cassandra #Mesos #Cordova #CloudStack #CouchDB #Hadoop #Spark #TrafficServer & more!
r1612633 importing geronimo jaxrs 2 and versioning it 7 (tomee) — rmannibucau
r1612632 HDFS-6720. Remove KeyProvider in EncryptionZoneManager. (wang) (hadoop) — wang
r1612631 HADOOP-10818. native client: refactor URI code to be clearer (cmccabe) (hadoop) — cmccabe
r1612630 fixing NOTICE (geronimo) — rmannibucau
r1612629 Merge r1612626 from trunk. YARN-2295. Refactored DistributedShell to use public APIs of protocol records. Contributed by Li Lu (hadoop) — jianhe
[PDFBOX-2226] IndexOutOfBoundsException when merging many PDFs in memory
An IndexOutOfBoundsException occurs when attempting to merge many (at least 10) PDF documents together. All PDFs exist in byte arrays...

[AURORA-595] test_cluster_option test case failing 
Test src.test.python.apache.aurora.common.test_cluster_option ..... FAILURE

E AttributeError: CaptureFixture instance has no attribute '_outerr'

[LUCENE-5825] Allowing the benchmarking algorithm to choose PostingsFormat
The algorithm file for benchmarking should allow PostingsFormat to be configurable.

[SENTRY-354] Test for update.distrib phase overriding
NOTE: I left the Fix Version blank because I think this should go into 1.5.0, i.e. after 1.4.0 is branched,...

[MESOS-1193] Check failed: promises.contains(containerId) crashes slave
This was observed with four slaves on one machine, one framework (Marathon) and around 100 tasks per slave.

I0404 17:58:58.298075...

Projects
HTTP Server
Abdera
Accumulo
ACE
ActiveMQ
Airavata
Allura
Ambari
Ant
Any23
APR
Archiva
Aries
Avro
Axis
Bigtop
Bloodhound
Buildr
BVal
Camel
Cassandra
Cayenne
Chemistry
Chukwa
Clerezza
CloudStack
Cocoon
Commons
Continuum
Cordova
CouchDB
Creadur
Crunch
cTAKES
Curator
CXF
DB
Deltacloud
DeltaSpike
DirectMemory
Directory
Empire-db
Etch
Felix
Flex
Flume
Forrest
Geronimo
Giraph
Gora
Gump
Hadoop
Hama
HBase
Helix
Hive
HttpComponents
Isis
Jackrabbit
James
jclouds
Jena
JMeter
JSPWiki
jUDDI
Kafka
Karaf
Knox
Lenya
Libcloud
Logging
Lucene
Lucene.Net
Lucy
Mahout
ManifoldCF
Marmotta
Maven
Mesos
MINA
MRUnit
MyFaces
Nutch
ODE
OFBiz
Olingo
Oltu
Onami
OODT
Oozie
Open Climate Workbench
OpenJPA
OpenMeetings
OpenNLP
OpenOffice
OpenWebBeans
PDFBox
Perl
Pig
Pivot
POI
Portals
Qpid
Rave
River
Roller
Santuario
ServiceMix
Shindig
Shiro
SIS
Sling
SpamAssassin
Spark
Sqoop
Stanbol
STeVe
Struts
Subversion
Synapse
Syncope
Tajo
Tapestry
Tcl
Thrift
Tika
Tiles
Tomcat
TomEE
Traffic Server
Turbine
Tuscany
UIMA
VCL
Velocity
Web Services
Whirr
Wicket
Wink
Wookie
Xalan
Xerces
XMLBeans
XML Graphics
ZooKeeper
Foundation
FAQ
Glossary
Licenses
Trademarks
News
Press Inquiries
Public Records
Mailing Lists
Sponsorship
Donations
Buy Stuff
Thanks
Contact
Foundation Projects
Attic
Conferences
Community Development
Incubator
Infrastructure
Labs
Legal Affairs
Public Relations
Security
Travel Assistance
The Apache Blogs

The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 

Forest Hill, MD –22 July 2014– The Apache Software...

The Apache Software Foundation Announces Apache™ Log4j™ v2
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture.

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer...

Apache Sentry architecture overview
Apache Sentry architecture overview

Apache Sentry is an authorization module for Hadoop that provides the granular, role-based authorization required to provide precise levels of access to the right users and...


Read More »
Copyright © 2012 The Apache Software Foundation, Licensed under the Apache License, Version 2.0.
Apache and the Apache feather logo are trademarks of The Apache Software Foundation.
The Apache Software Foundation
The Apache Software Foundation

Community-led development since 1999.
FoundationProjectsPeopleGet InvolvedDownloadSupport ApacheHome  
We consider ourselves
not simply a group of projects sharing a server, but rather a community of developers and users.
The Apache Software Foundation
provides support for the Apache community of open-source software projects, which provide software products for the public good.
The Apache projects are defined
by collaborative consensus based processes, an open, pragmatic software license and a desire to create high quality software that leads the way in its field.
Featured Projects »Apache TomEE Apache OpenMeetings Apache Tobago
The ASF is made up of nearly 150
top level projects which cover a wide range of technologies. Chances are if you are looking for a rewarding experience in Open Source, you are going to find it here.
Apache TomEE
Apache TomEE is an all-Apache Java EE 6 Web Profile stack for Apache Tomcat
Apache TomEE Web Profile delivers Servlets, JSP, JSF, JTA, JPA, CDI, Bean Validation and EJB Lite. Apache TomEE Plus has all the features of TomEE with the addition of JAX-RS (RESTfull Services), JAX-WS (Web Services), JMS (Java Message Service) and JCA (the Java Connector Architecture). The additional functionality is delivered via Apache CXF, Apache ActiveMQ and the Geronimo Connector library
Latest News
If you would like to keep up with news and announcements from the foundation and all its projects, you can subscribe to the Apache Announcements List or follow the Foundation Blog.

The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open Source projects and initiatives, announced today that...
The Apache Software Foundation Announces Apache™ Log4j™ v2
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture.

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open Source projects and initiatives, announced today the General Availability of Apache™...
Travel Assistance Applications Now Open for ApacheCon Europe: Budapest 17-21 November 2014
The Apache Software Foundation (ASF)'s Travel Assistance Committee (TAC) is now accepting applications for ApacheCon Europe 2014, taking place 17-21 November in Budapest, Hungary.

The TAC is seeking individuals from the Apache community at-large, users, developers, educators, students, Committers, and Members, who would like to attend ApacheCon, but need some...
The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
> This announcement is also available online at http://s.apache.org/keG
 
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 
 
Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of...
The Apache Software Foundation Announces Apache™ Log4j™ v2
> This announcement is also available online at http://s.apache.org/wm
 
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture. 
 
Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 170 Open...
[ANNOUNCE] Apache Jackrabbit Oak 1.0.2 released
The Apache Jackrabbit community is pleased to announce the release of Apache Jackrabbit Oak 1.0.2. The release is available for download at:
 
http://jackrabbit.apache.org/downloads.html
 
See the full release notes below for details about this release.
 
Release Notes -- Apache Jackrabbit Oak -- Version 1.0.2
 
Introduction ------------
 ...
Latest Activity
This is an overview of activity going on with our projects. SVN commits, bug reports, tweets, you name it.

@TheASF: The Apache Software Foundation Announces #Apache™ #Tez™ as a Top-Level Project http://t.co/P6cJ5nbcMQ #OpenSource #BigData #Hadoop #YARN
@TheASF: The Apache Software Foundation Announces #Apache™ #Log4j™ v2 http://t.co/zhk0cTIzAf #OpenSource #logging #library #Java #plugin
@TheASF: Join #Apache @OSCON: Booth 13 + presos on #HTTP Server #Cassandra #Mesos #Cordova #CloudStack #CouchDB #Hadoop #Spark #TrafficServer & more!
r1612633 importing geronimo jaxrs 2 and versioning it 7 (tomee) — rmannibucau
r1612632 HDFS-6720. Remove KeyProvider in EncryptionZoneManager. (wang) (hadoop) — wang
r1612631 HADOOP-10818. native client: refactor URI code to be clearer (cmccabe) (hadoop) — cmccabe
r1612630 fixing NOTICE (geronimo) — rmannibucau
r1612629 Merge r1612626 from trunk. YARN-2295. Refactored DistributedShell to use public APIs of protocol records. Contributed by Li Lu (hadoop) — jianhe
[PDFBOX-2226] IndexOutOfBoundsException when merging many PDFs in memory
An IndexOutOfBoundsException occurs when attempting to merge many (at least 10) PDF documents together. All PDFs exist in byte arrays...

[AURORA-595] test_cluster_option test case failing 
Test src.test.python.apache.aurora.common.test_cluster_option ..... FAILURE

E AttributeError: CaptureFixture instance has no attribute '_outerr'

[LUCENE-5825] Allowing the benchmarking algorithm to choose PostingsFormat
The algorithm file for benchmarking should allow PostingsFormat to be configurable.

[SENTRY-354] Test for update.distrib phase overriding
NOTE: I left the Fix Version blank because I think this should go into 1.5.0, i.e. after 1.4.0 is branched,...

[MESOS-1193] Check failed: promises.contains(containerId) crashes slave
This was observed with four slaves on one machine, one framework (Marathon) and around 100 tasks per slave.

I0404 17:58:58.298075...

Projects
HTTP Server
Abdera
Accumulo
ACE
ActiveMQ
Airavata
Allura
Ambari
Ant
Any23
APR
Archiva
Aries
Avro
Axis
Bigtop
Bloodhound
Buildr
BVal
Camel
Cassandra
Cayenne
Chemistry
Chukwa
Clerezza
CloudStack
Cocoon
Commons
Continuum
Cordova
CouchDB
Creadur
Crunch
cTAKES
Curator
CXF
DB
Deltacloud
DeltaSpike
DirectMemory
Directory
Empire-db
Etch
Felix
Flex
Flume
Forrest
Geronimo
Giraph
Gora
Gump
Hadoop
Hama
HBase
Helix
Hive
HttpComponents
Isis
Jackrabbit
James
jclouds
Jena
JMeter
JSPWiki
jUDDI
Kafka
Karaf
Knox
Lenya
Libcloud
Logging
Lucene
Lucene.Net
Lucy
Mahout
ManifoldCF
Marmotta
Maven
Mesos
MINA
MRUnit
MyFaces
Nutch
ODE
OFBiz
Olingo
Oltu
Onami
OODT
Oozie
Open Climate Workbench
OpenJPA
OpenMeetings
OpenNLP
OpenOffice
OpenWebBeans
PDFBox
Perl
Pig
Pivot
POI
Portals
Qpid
Rave
River
Roller
Santuario
ServiceMix
Shindig
Shiro
SIS
Sling
SpamAssassin
Spark
Sqoop
Stanbol
STeVe
Struts
Subversion
Synapse
Syncope
Tajo
Tapestry
Tcl
Thrift
Tika
Tiles
Tomcat
TomEE
Traffic Server
Turbine
Tuscany
UIMA
VCL
Velocity
Web Services
Whirr
Wicket
Wink
Wookie
Xalan
Xerces
XMLBeans
XML Graphics
ZooKeeper
Foundation
FAQ
Glossary
Licenses
Trademarks
News
Press Inquiries
Public Records
Mailing Lists
Sponsorship
Donations
Buy Stuff
Thanks
Contact
Foundation Projects
Attic
Conferences
Community Development
Incubator
Infrastructure
Labs
Legal Affairs
Public Relations
Security
Travel Assistance
The Apache Blogs

The Apache Software Foundation Announces Apache™ Tez™ as a Top-Level Project
Highly-efficient Open Source framework for Apache Hadoop® YARN-powered data processing applications in use at Microsoft, NASA, Netflix, and Yahoo, among others. 

Forest Hill, MD –22 July 2014– The Apache Software...

The Apache Software Foundation Announces Apache™ Log4j™ v2
Framework for widely-used Open Source Java-based logging library now faster and more extensible, with new plugin architecture.

Forest Hill, MD –22 July 2014– The Apache Software Foundation (ASF), the all-volunteer...

Apache Sentry architecture overview
Apache Sentry architecture overview

Apache Sentry is an authorization module for Hadoop that provides the granular, role-based authorization required to provide precise levels of access to the right users and...


Read More »
Copyright © 2012 The Apache Software Foundation, Licensed under the Apache License, Version 2.0.
Apache and the Apache feather logo are trademarks of The Apache Software Foundation.

Welcome to Wikipedia,
the free encyclopedia that anyone can edit.
4,562,552 articles in English
Arts
Biography
Geography
History
Mathematics
Science
Society
Technology
All portals
From today's featured article
Peat workings on Chat Moss
Chat Moss is a large area of peat bog that makes up 30 per cent of the City of Salford, in Greater Manchester, England. North of the River Irwell, 5 miles (8 km) to the west of Manchester, it occupies an area of about 10.6 square miles (27.5 km2). Peat development seems to have begun there at the end of the last ice age, about 10,000 years ago, and the depth of peat ranges from 24 to 30 feet (7 to 9 m). A great deal of reclamation work has been carried out, but a large network of drainage channels is required to keep it from reverting to bog. In 1958 peat extractors discovered the severed head of what is believed to be a Romano-British Celt, possibly a sacrificial victim. Much of Chat Moss is now prime agricultural land, although farming in the area is in decline. A 228-acre (92 ha) area of Chat Moss, notified as Astley and Bedford Mosses, has been designated a Site of Special Scientific Interest. Chat Moss threatened the completion of the Liverpool and Manchester Railway, until George Stephenson succeeded in constructing a railway line through it in 1829; his solution was to "float" the line on a bed of bound heather and branches topped with tar and covered with rubble stone. (Full article...)

Recently featured: Leo Minor – Australian contribution to the Battle of Normandy – McDonald's Cycle Center

Archive – By email – More featured articles...
Did you know...
From Wikipedia's new and recently improved content:

Paratropis tuxtlensis

... that Paratropis tuxtlensis (pictured), a newly discovered species of spider, coats its body in soil, apparently to conceal itself?
... that Michael Botticelli is the first director of the White House Office of National Drug Control Policy to state that he is in recovery for a substance abuse problem?
... that "Jaga Dia Untukku", a song by Siti Nurhaliza, was inspired by feelings about her husband's motorcycle accident?
... that Émilie Tremblay grew radishes and lettuces on the sod roof of her cabin in the Yukon?
... that the short-lived MGM-Pathé Communications was formed in 1990 when Giancarlo Parretti purchased MGM/UA Communications Inc. and merged it with his Pathé Communications Group?
... that after Arsenal's defeat in the 2005 FA Community Shield, manager Arsène Wenger stated, "no-one regards it as a trophy so I do not mind anymore"?
... that on Kuntry Livin', recording artist Big Smo blends country music and hip hop in a style known as hick hop?
Archive – Start a new article – Nominate an article
In the news
Aircraft used for Malaysia Airlines Flight 17
Militants attack a checkpoint in Egypt's western desert region, killing 22 soldiers.
In golf, Rory McIlroy wins the Open Championship.
The Cook Islands Party retains its majority in the Cook Islands general election.
Malaysia Airlines Flight 17 (aircraft pictured), en route from Amsterdam to Kuala Lumpur, is shot down in eastern Ukraine, killing all 298 people on board.
Israel launches a ground invasion of the Gaza Strip.
Militants attack two checkpoints in the Chaambi Mountains, Tunisia, killing fourteen soldiers.
Typhoon Rammasun causes at least 94 deaths in the Philippines and at least 18 in China.
The BRICS summit is held in Brazil.
Ongoing: Ukrainian unrest
Recent deaths: James Garner – Elaine Stritch – Karl Albrecht – Johnny Winter

On this day...
July 22: Feast day of Mary Magdalene; Pi Approximation Day

Wiley Post

838 – Arab–Byzantine wars: The forces of the Abbasid Caliphate defeated Byzantine Empire troops, led by Emperor Theophilos himself, at the Battle of Anzen near present-day Dazman, Turkey.
1864 – American Civil War: Confederate forces unsuccessfully attacked Union troops at the Battle of Atlanta.
1933 – Wiley Post (pictured) became the first pilot to fly solo around the world, landing after a seven-day, nineteen-hour flight at Floyd Bennett Field in Brooklyn, New York City.
1992 – Colombian drug lord Pablo Escobar escaped from his luxurious private prison and spent the next 17 months on the run.
2011 – Two sequential terrorist attacks in Oslo and Utøya claimed the lives of 77 people in the deadliest attack in Norway since World War II.
More anniversaries: July 21 – July 22 – July 23

Archive – By email – List of historical anniversaries
It is now July 22, 2014 (UTC) – Reload this page
Today's featured picture
 Male and female
A female (top) and male orange-bellied parrot (Neophema chrysogaster), photographed in Melaleuca, Tasmania, Australia. First described by John Latham, this small parrot breeds only in South West Tasmania. Fewer than 50 individuals are known in the wild, and the species is considered critically endangered, though there is a captive breeding population.

Photograph: JJ Harrison
Recently featured: Beppe Grillo – Crew of Apollo 11 – Soldier fly

Archive – More featured pictures...
Other areas of Wikipedia
Community portal – Bulletin board, projects, resources and activities covering a wide range of Wikipedia areas.
Help desk – Ask questions about using Wikipedia.
Local embassy – For Wikipedia-related communication in languages other than English.
Reference desk – Serving as virtual librarians, Wikipedia volunteers tackle your questions on a wide range of subjects.
Site news – Announcements, updates, articles and press releases on Wikipedia and the Wikimedia Foundation.
Village pump – For discussions about Wikipedia itself, including areas for technical issues and policies.
Wikipedia's sister projects
Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects:

Commons	Commons
Free media repository	MediaWiki	MediaWiki
Wiki software development	Meta-Wiki	Meta-Wiki
Wikimedia project coordination
Wikibooks	Wikibooks
Free textbooks and manuals	Wikidata	Wikidata
Free knowledge base	Wikinews	Wikinews
Free-content news
Wikiquote	Wikiquote
Collection of quotations	Wikisource	Wikisource
Free-content library	Wikispecies	Wikispecies
Directory of species
Wikiversity	Wikiversity
Free learning materials and activities	Wikivoyage	Wikivoyage
Free travel guide	Wiktionary	Wiktionary
Dictionary and thesaurus
Wikipedia languages
This Wikipedia is written in English. Started in 2001, it currently contains 4,562,552 articles. Many other Wikipedias are available; some of the largest are listed below.

More than 1,000,000 articles: Deutsch español français italiano Nederlands polski ??????? svenska
More than 400,000 articles: català ??? norsk bokmål português Ti?ng Vi?t ?????????? ?? ?????
More than 200,000 articles: ??????? Bahasa Indonesia Bahasa Melayu ceština ?????? / srpski ??? magyar româna suomi Türkçe
More than 50,000 articles: ????????? dansk eesti ???????? English (simple) Esperanto euskara galego ????? hrvatski latviešu lietuviu norsk nynorsk slovencina slovenšcina srpskohrvatski / ?????????????? ???
Complete list of Wikipedias
Navigation menu
Create accountLog inMain PageTalkReadView sourceView history

Main page
Contents
Featured content
Current events
Random article
Donate to Wikipedia
Wikimedia Shop
Interaction
Help
About Wikipedia
Community portal
Recent changes
Contact page
Tools
What links here
Related changes
Upload file
Special pages
Permanent link
Page information
Wikidata item
Print/export
Create a book
Download as PDF
Printable version
Languages
Simple English
???????
Bahasa Indonesia
Bahasa Melayu
?????????
Català
Ceština
Dansk
Deutsch
Eesti
????????
Español
Esperanto
Euskara
?????
Français
Galego
???
?????
Hrvatski
Italiano
???????
Latviešu
Lietuviu
Magyar
Nederlands
???
Norsk bokmål
Norsk nynorsk
Polski
Português
Româna
???????
Slovencina
Slovenšcina
?????? / srpski
Srpskohrvatski / ??????????????
Suomi
Svenska
???
Ti?ng Vi?t
Türkçe
??????????
??
Complete list
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaDevelopersMobile viewWikimedia Foundation Powered by MediaWiki

Welcome to Wikipedia,
the free encyclopedia that anyone can edit.
4,562,552 articles in English
Arts
Biography
Geography
History
Mathematics
Science
Society
Technology
All portals
From today's featured article
Peat workings on Chat Moss
Chat Moss is a large area of peat bog that makes up 30 per cent of the City of Salford, in Greater Manchester, England. North of the River Irwell, 5 miles (8 km) to the west of Manchester, it occupies an area of about 10.6 square miles (27.5 km2). Peat development seems to have begun there at the end of the last ice age, about 10,000 years ago, and the depth of peat ranges from 24 to 30 feet (7 to 9 m). A great deal of reclamation work has been carried out, but a large network of drainage channels is required to keep it from reverting to bog. In 1958 peat extractors discovered the severed head of what is believed to be a Romano-British Celt, possibly a sacrificial victim. Much of Chat Moss is now prime agricultural land, although farming in the area is in decline. A 228-acre (92 ha) area of Chat Moss, notified as Astley and Bedford Mosses, has been designated a Site of Special Scientific Interest. Chat Moss threatened the completion of the Liverpool and Manchester Railway, until George Stephenson succeeded in constructing a railway line through it in 1829; his solution was to "float" the line on a bed of bound heather and branches topped with tar and covered with rubble stone. (Full article...)

Recently featured: Leo Minor – Australian contribution to the Battle of Normandy – McDonald's Cycle Center

Archive – By email – More featured articles...
Did you know...
From Wikipedia's new and recently improved content:

Paratropis tuxtlensis

... that Paratropis tuxtlensis (pictured), a newly discovered species of spider, coats its body in soil, apparently to conceal itself?
... that Michael Botticelli is the first director of the White House Office of National Drug Control Policy to state that he is in recovery for a substance abuse problem?
... that "Jaga Dia Untukku", a song by Siti Nurhaliza, was inspired by feelings about her husband's motorcycle accident?
... that Émilie Tremblay grew radishes and lettuces on the sod roof of her cabin in the Yukon?
... that the short-lived MGM-Pathé Communications was formed in 1990 when Giancarlo Parretti purchased MGM/UA Communications Inc. and merged it with his Pathé Communications Group?
... that after Arsenal's defeat in the 2005 FA Community Shield, manager Arsène Wenger stated, "no-one regards it as a trophy so I do not mind anymore"?
... that on Kuntry Livin', recording artist Big Smo blends country music and hip hop in a style known as hick hop?
Archive – Start a new article – Nominate an article
In the news
Aircraft used for Malaysia Airlines Flight 17
Militants attack a checkpoint in Egypt's western desert region, killing 22 soldiers.
In golf, Rory McIlroy wins the Open Championship.
The Cook Islands Party retains its majority in the Cook Islands general election.
Malaysia Airlines Flight 17 (aircraft pictured), en route from Amsterdam to Kuala Lumpur, is shot down in eastern Ukraine, killing all 298 people on board.
Israel launches a ground invasion of the Gaza Strip.
Militants attack two checkpoints in the Chaambi Mountains, Tunisia, killing fourteen soldiers.
Typhoon Rammasun causes at least 94 deaths in the Philippines and at least 18 in China.
The BRICS summit is held in Brazil.
Ongoing: Ukrainian unrest
Recent deaths: James Garner – Elaine Stritch – Karl Albrecht – Johnny Winter

On this day...
July 22: Feast day of Mary Magdalene; Pi Approximation Day

Wiley Post

838 – Arab–Byzantine wars: The forces of the Abbasid Caliphate defeated Byzantine Empire troops, led by Emperor Theophilos himself, at the Battle of Anzen near present-day Dazman, Turkey.
1864 – American Civil War: Confederate forces unsuccessfully attacked Union troops at the Battle of Atlanta.
1933 – Wiley Post (pictured) became the first pilot to fly solo around the world, landing after a seven-day, nineteen-hour flight at Floyd Bennett Field in Brooklyn, New York City.
1992 – Colombian drug lord Pablo Escobar escaped from his luxurious private prison and spent the next 17 months on the run.
2011 – Two sequential terrorist attacks in Oslo and Utøya claimed the lives of 77 people in the deadliest attack in Norway since World War II.
More anniversaries: July 21 – July 22 – July 23

Archive – By email – List of historical anniversaries
It is now July 22, 2014 (UTC) – Reload this page
Today's featured picture
 Male and female
A female (top) and male orange-bellied parrot (Neophema chrysogaster), photographed in Melaleuca, Tasmania, Australia. First described by John Latham, this small parrot breeds only in South West Tasmania. Fewer than 50 individuals are known in the wild, and the species is considered critically endangered, though there is a captive breeding population.

Photograph: JJ Harrison
Recently featured: Beppe Grillo – Crew of Apollo 11 – Soldier fly

Archive – More featured pictures...
Other areas of Wikipedia
Community portal – Bulletin board, projects, resources and activities covering a wide range of Wikipedia areas.
Help desk – Ask questions about using Wikipedia.
Local embassy – For Wikipedia-related communication in languages other than English.
Reference desk – Serving as virtual librarians, Wikipedia volunteers tackle your questions on a wide range of subjects.
Site news – Announcements, updates, articles and press releases on Wikipedia and the Wikimedia Foundation.
Village pump – For discussions about Wikipedia itself, including areas for technical issues and policies.
Wikipedia's sister projects
Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects:

Commons	Commons
Free media repository	MediaWiki	MediaWiki
Wiki software development	Meta-Wiki	Meta-Wiki
Wikimedia project coordination
Wikibooks	Wikibooks
Free textbooks and manuals	Wikidata	Wikidata
Free knowledge base	Wikinews	Wikinews
Free-content news
Wikiquote	Wikiquote
Collection of quotations	Wikisource	Wikisource
Free-content library	Wikispecies	Wikispecies
Directory of species
Wikiversity	Wikiversity
Free learning materials and activities	Wikivoyage	Wikivoyage
Free travel guide	Wiktionary	Wiktionary
Dictionary and thesaurus
Wikipedia languages
This Wikipedia is written in English. Started in 2001, it currently contains 4,562,552 articles. Many other Wikipedias are available; some of the largest are listed below.

More than 1,000,000 articles: Deutsch español français italiano Nederlands polski ??????? svenska
More than 400,000 articles: català ??? norsk bokmål português Ti?ng Vi?t ?????????? ?? ?????
More than 200,000 articles: ??????? Bahasa Indonesia Bahasa Melayu ceština ?????? / srpski ??? magyar româna suomi Türkçe
More than 50,000 articles: ????????? dansk eesti ???????? English (simple) Esperanto euskara galego ????? hrvatski latviešu lietuviu norsk nynorsk slovencina slovenšcina srpskohrvatski / ?????????????? ???
Complete list of Wikipedias
Navigation menu
Create accountLog inMain PageTalkReadView sourceView history

Main page
Contents
Featured content
Current events
Random article
Donate to Wikipedia
Wikimedia Shop
Interaction
Help
About Wikipedia
Community portal
Recent changes
Contact page
Tools
What links here
Related changes
Upload file
Special pages
Permanent link
Page information
Wikidata item
Print/export
Create a book
Download as PDF
Printable version
Languages
Simple English
???????
Bahasa Indonesia
Bahasa Melayu
?????????
Català
Ceština
Dansk
Deutsch
Eesti
????????
Español
Esperanto
Euskara
?????
Français
Galego
???
?????
Hrvatski
Italiano
???????
Latviešu
Lietuviu
Magyar
Nederlands
???
Norsk bokmål
Norsk nynorsk
Polski
Português
Româna
???????
Slovencina
Slovenšcina
?????? / srpski
Srpskohrvatski / ??????????????
Suomi
Svenska
???
Ti?ng Vi?t
Türkçe
??????????
??
Complete list
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaDevelopersMobile viewWikimedia Foundation Powered by MediaWiki

Welcome to Wikipedia,
the free encyclopedia that anyone can edit.
4,562,552 articles in English
Arts
Biography
Geography
History
Mathematics
Science
Society
Technology
All portals
From today's featured article
Peat workings on Chat Moss
Chat Moss is a large area of peat bog that makes up 30 per cent of the City of Salford, in Greater Manchester, England. North of the River Irwell, 5 miles (8 km) to the west of Manchester, it occupies an area of about 10.6 square miles (27.5 km2). Peat development seems to have begun there at the end of the last ice age, about 10,000 years ago, and the depth of peat ranges from 24 to 30 feet (7 to 9 m). A great deal of reclamation work has been carried out, but a large network of drainage channels is required to keep it from reverting to bog. In 1958 peat extractors discovered the severed head of what is believed to be a Romano-British Celt, possibly a sacrificial victim. Much of Chat Moss is now prime agricultural land, although farming in the area is in decline. A 228-acre (92 ha) area of Chat Moss, notified as Astley and Bedford Mosses, has been designated a Site of Special Scientific Interest. Chat Moss threatened the completion of the Liverpool and Manchester Railway, until George Stephenson succeeded in constructing a railway line through it in 1829; his solution was to "float" the line on a bed of bound heather and branches topped with tar and covered with rubble stone. (Full article...)

Recently featured: Leo Minor – Australian contribution to the Battle of Normandy – McDonald's Cycle Center

Archive – By email – More featured articles...
Did you know...
From Wikipedia's new and recently improved content:

Paratropis tuxtlensis

... that Paratropis tuxtlensis (pictured), a newly discovered species of spider, coats its body in soil, apparently to conceal itself?
... that Michael Botticelli is the first director of the White House Office of National Drug Control Policy to state that he is in recovery for a substance abuse problem?
... that "Jaga Dia Untukku", a song by Siti Nurhaliza, was inspired by feelings about her husband's motorcycle accident?
... that Émilie Tremblay grew radishes and lettuces on the sod roof of her cabin in the Yukon?
... that the short-lived MGM-Pathé Communications was formed in 1990 when Giancarlo Parretti purchased MGM/UA Communications Inc. and merged it with his Pathé Communications Group?
... that after Arsenal's defeat in the 2005 FA Community Shield, manager Arsène Wenger stated, "no-one regards it as a trophy so I do not mind anymore"?
... that on Kuntry Livin', recording artist Big Smo blends country music and hip hop in a style known as hick hop?
Archive – Start a new article – Nominate an article
In the news
Aircraft used for Malaysia Airlines Flight 17
Militants attack a checkpoint in Egypt's western desert region, killing 22 soldiers.
In golf, Rory McIlroy wins the Open Championship.
The Cook Islands Party retains its majority in the Cook Islands general election.
Malaysia Airlines Flight 17 (aircraft pictured), en route from Amsterdam to Kuala Lumpur, is shot down in eastern Ukraine, killing all 298 people on board.
Israel launches a ground invasion of the Gaza Strip.
Militants attack two checkpoints in the Chaambi Mountains, Tunisia, killing fourteen soldiers.
Typhoon Rammasun causes at least 94 deaths in the Philippines and at least 18 in China.
The BRICS summit is held in Brazil.
Ongoing: Ukrainian unrest
Recent deaths: James Garner – Elaine Stritch – Karl Albrecht – Johnny Winter

On this day...
July 22: Feast day of Mary Magdalene; Pi Approximation Day

Wiley Post

838 – Arab–Byzantine wars: The forces of the Abbasid Caliphate defeated Byzantine Empire troops, led by Emperor Theophilos himself, at the Battle of Anzen near present-day Dazman, Turkey.
1864 – American Civil War: Confederate forces unsuccessfully attacked Union troops at the Battle of Atlanta.
1933 – Wiley Post (pictured) became the first pilot to fly solo around the world, landing after a seven-day, nineteen-hour flight at Floyd Bennett Field in Brooklyn, New York City.
1992 – Colombian drug lord Pablo Escobar escaped from his luxurious private prison and spent the next 17 months on the run.
2011 – Two sequential terrorist attacks in Oslo and Utøya claimed the lives of 77 people in the deadliest attack in Norway since World War II.
More anniversaries: July 21 – July 22 – July 23

Archive – By email – List of historical anniversaries
It is now July 22, 2014 (UTC) – Reload this page
Today's featured picture
 Male and female
A female (top) and male orange-bellied parrot (Neophema chrysogaster), photographed in Melaleuca, Tasmania, Australia. First described by John Latham, this small parrot breeds only in South West Tasmania. Fewer than 50 individuals are known in the wild, and the species is considered critically endangered, though there is a captive breeding population.

Photograph: JJ Harrison
Recently featured: Beppe Grillo – Crew of Apollo 11 – Soldier fly

Archive – More featured pictures...
Other areas of Wikipedia
Community portal – Bulletin board, projects, resources and activities covering a wide range of Wikipedia areas.
Help desk – Ask questions about using Wikipedia.
Local embassy – For Wikipedia-related communication in languages other than English.
Reference desk – Serving as virtual librarians, Wikipedia volunteers tackle your questions on a wide range of subjects.
Site news – Announcements, updates, articles and press releases on Wikipedia and the Wikimedia Foundation.
Village pump – For discussions about Wikipedia itself, including areas for technical issues and policies.
Wikipedia's sister projects
Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects:

Commons	Commons
Free media repository	MediaWiki	MediaWiki
Wiki software development	Meta-Wiki	Meta-Wiki
Wikimedia project coordination
Wikibooks	Wikibooks
Free textbooks and manuals	Wikidata	Wikidata
Free knowledge base	Wikinews	Wikinews
Free-content news
Wikiquote	Wikiquote
Collection of quotations	Wikisource	Wikisource
Free-content library	Wikispecies	Wikispecies
Directory of species
Wikiversity	Wikiversity
Free learning materials and activities	Wikivoyage	Wikivoyage
Free travel guide	Wiktionary	Wiktionary
Dictionary and thesaurus
Wikipedia languages
This Wikipedia is written in English. Started in 2001, it currently contains 4,562,552 articles. Many other Wikipedias are available; some of the largest are listed below.

More than 1,000,000 articles: Deutsch español français italiano Nederlands polski ??????? svenska
More than 400,000 articles: català ??? norsk bokmål português Ti?ng Vi?t ?????????? ?? ?????
More than 200,000 articles: ??????? Bahasa Indonesia Bahasa Melayu ceština ?????? / srpski ??? magyar româna suomi Türkçe
More than 50,000 articles: ????????? dansk eesti ???????? English (simple) Esperanto euskara galego ????? hrvatski latviešu lietuviu norsk nynorsk slovencina slovenšcina srpskohrvatski / ?????????????? ???
Complete list of Wikipedias
Navigation menu
Create accountLog inMain PageTalkReadView sourceView history

Main page
Contents
Featured content
Current events
Random article
Donate to Wikipedia
Wikimedia Shop
Interaction
Help
About Wikipedia
Community portal
Recent changes
Contact page
Tools
What links here
Related changes
Upload file
Special pages
Permanent link
Page information
Wikidata item
Print/export
Create a book
Download as PDF
Printable version
Languages
Simple English
???????
Bahasa Indonesia
Bahasa Melayu
?????????
Català
Ceština
Dansk
Deutsch
Eesti
????????
Español
Esperanto
Euskara
?????
Français
Galego
???
?????
Hrvatski
Italiano
???????
Latviešu
Lietuviu
Magyar
Nederlands
???
Norsk bokmål
Norsk nynorsk
Polski
Português
Româna
???????
Slovencina
Slovenšcina
?????? / srpski
Srpskohrvatski / ??????????????
Suomi
Svenska
???
Ti?ng Vi?t
Türkçe
??????????
??
Complete list
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaDevelopersMobile viewWikimedia Foundation Powered by MediaWiki